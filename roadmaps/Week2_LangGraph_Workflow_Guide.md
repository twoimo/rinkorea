# Week 2: LangGraph Q&A ìë™í™” ì›Œí¬í”Œë¡œìš°

## ğŸ¯ **ëª©í‘œ**
**LangGraph ìƒíƒœ ë¨¸ì‹ **ì„ í™œìš©í•œ ì§€ëŠ¥í˜• Q&A ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶•

---

## ğŸ“‹ **Day 8-9: Q&A ì‹œìŠ¤í…œ ë¶„ì„**

### **1. ê¸°ì¡´ Q&A ë°ì´í„° ë¶„ì„**

```python
# backend/ai-service/analysis/qna_analyzer.py
import pandas as pd
import asyncio
from collections import Counter
from database.supabase_client import SupabaseClient

class QnAAnalyzer:
    def __init__(self):
        self.supabase = SupabaseClient()
        
    async def analyze_existing_qna(self):
        """ê¸°ì¡´ Q&A ë°ì´í„° ë¶„ì„"""
        # ê¸°ì¡´ Q&A ë°ì´í„° ë¡œë“œ
        inquiries = await self.supabase.fetch_inquiries()
        
        analysis_result = {
            "total_count": len(inquiries),
            "status_distribution": Counter([q["status"] for q in inquiries]),
            "category_analysis": self._categorize_inquiries(inquiries),
            "response_time_analysis": self._analyze_response_times(inquiries),
            "common_keywords": self._extract_keywords(inquiries)
        }
        
        return analysis_result
    
    def _categorize_inquiries(self, inquiries):
        """ë¬¸ì˜ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜"""
        categories = {
            "ì œí’ˆ_ë¬¸ì˜": ["ì œí’ˆ", "ë¦°ì½”íŠ¸", "ë¦°í•˜ë“œ", "ë°©ìˆ˜", "ì½”íŒ…"],
            "ê¸°ìˆ _ì§€ì›": ["ì‹œê³µ", "ì‚¬ìš©ë²•", "ì ìš©", "ë°©ë²•", "ê¸°ìˆ "],
            "ê²¬ì _ìš”ì²­": ["ê²¬ì ", "ê°€ê²©", "ë¹„ìš©", "êµ¬ë§¤", "ì£¼ë¬¸"],
            "ì¼ë°˜_ë¬¸ì˜": ["ë¬¸ì˜", "ì—°ë½", "ìœ„ì¹˜", "íšŒì‚¬"]
        }
        
        categorized = {cat: 0 for cat in categories.keys()}
        
        for inquiry in inquiries:
            content = inquiry["title"] + " " + inquiry["content"]
            for category, keywords in categories.items():
                if any(keyword in content for keyword in keywords):
                    categorized[category] += 1
                    break
        
        return categorized
    
    def _extract_keywords(self, inquiries):
        """ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        from collections import Counter
        import re
        
        all_text = " ".join([
            inquiry["title"] + " " + inquiry["content"] 
            for inquiry in inquiries
        ])
        
        # í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ (2ê¸€ì ì´ìƒ)
        korean_words = re.findall(r'[ê°€-í£]{2,}', all_text)
        return Counter(korean_words).most_common(20)
```

### **2. ìë™ ë‹µë³€ í…œí”Œë¦¿ ìƒì„±**

```python
# backend/ai-service/templates/response_templates.py
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
import os

class ResponseTemplateGenerator:
    def __init__(self):
        self.llm = ChatOpenAI(
            temperature=0.3,
            model="gpt-4",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        self.template_categories = {
            "ì œí’ˆ_ë¬¸ì˜": {
                "system_prompt": "ë¦°ì½”ë¦¬ì•„ ì œí’ˆ ì „ë¬¸ê°€ë¡œì„œ ë‹µë³€í•©ë‹ˆë‹¤.",
                "context_needed": ["product_info", "specifications"],
                "tone": "ì „ë¬¸ì ì´ê³  ìƒì„¸í•œ"
            },
            "ê¸°ìˆ _ì§€ì›": {
                "system_prompt": "ê¸°ìˆ  ì§€ì› ì „ë¬¸ê°€ë¡œì„œ ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•©ë‹ˆë‹¤.",
                "context_needed": ["technical_docs", "installation_guide"],
                "tone": "ì¹œì ˆí•˜ê³  ì‹¤ìš©ì ì¸"
            },
            "ê²¬ì _ìš”ì²­": {
                "system_prompt": "ì˜ì—… ë‹´ë‹¹ìë¡œì„œ ê²¬ì  ì •ë³´ë¥¼ ì•ˆë‚´í•©ë‹ˆë‹¤.",
                "context_needed": ["pricing_info", "contact_info"],
                "tone": "ë¹„ì¦ˆë‹ˆìŠ¤ ì¹œí™”ì ì¸"
            }
        }

    async def generate_response_template(self, category: str, inquiry_text: str):
        """ì¹´í…Œê³ ë¦¬ë³„ ë§ì¶¤ ë‹µë³€ í…œí”Œë¦¿ ìƒì„±"""
        category_config = self.template_categories.get(category, self.template_categories["ì œí’ˆ_ë¬¸ì˜"])
        
        prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ {system_prompt}
        
        ë‹¤ìŒ ê³ ê° ë¬¸ì˜ì— ëŒ€í•œ {tone} ë‹µë³€ í…œí”Œë¦¿ì„ ìƒì„±í•´ì£¼ì„¸ìš”:
        
        ë¬¸ì˜ ë‚´ìš©: {inquiry}
        
        ë‹µë³€ í…œí”Œë¦¿ì€ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:
        1. ì¸ì‚¬ë§
        2. ë¬¸ì˜ ë‚´ìš© í™•ì¸
        3. êµ¬ì²´ì ì¸ ë‹µë³€/ì •ë³´ ì œê³µ
        4. ì¶”ê°€ ì§€ì› ì•ˆë‚´
        5. ë§ˆë¬´ë¦¬ ì¸ì‚¬
        
        í…œí”Œë¦¿:
        """)
        
        response = await self.llm.ainvoke({
            "system_prompt": category_config["system_prompt"],
            "tone": category_config["tone"],
            "inquiry": inquiry_text
        })
        
        return response.content
```

---

## ğŸ“‹ **Day 10-11: LangGraph ì›Œí¬í”Œë¡œìš°**

### **1. ìƒíƒœ ì •ì˜ ë° ë…¸ë“œ êµ¬ì„±**

```python
# backend/ai-service/workflows/qna_workflow.py
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from typing import TypedDict, Annotated, List
import operator
import os

class QnAState(TypedDict):
    inquiry_id: str
    inquiry_text: str
    user_info: dict
    category: str
    urgency_level: str
    context_documents: List[str]
    generated_response: str
    requires_human_review: bool
    confidence_score: float
    next_action: str

class QnAWorkflow:
    def __init__(self):
        self.llm = ChatOpenAI(
            temperature=0.7,
            model="gpt-4",
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±
        self.workflow = StateGraph(QnAState)
        self._build_workflow()

    def _build_workflow(self):
        """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì¶•"""
        # ë…¸ë“œ ì¶”ê°€
        self.workflow.add_node("classify_inquiry", self.classify_inquiry)
        self.workflow.add_node("assess_urgency", self.assess_urgency)
        self.workflow.add_node("retrieve_context", self.retrieve_context)
        self.workflow.add_node("generate_response", self.generate_response)
        self.workflow.add_node("review_response", self.review_response)
        self.workflow.add_node("send_response", self.send_response)
        self.workflow.add_node("escalate_to_human", self.escalate_to_human)
        
        # ì‹œì‘ì  ì„¤ì •
        self.workflow.set_entry_point("classify_inquiry")
        
        # ì—£ì§€ ì •ì˜
        self.workflow.add_edge("classify_inquiry", "assess_urgency")
        self.workflow.add_edge("assess_urgency", "retrieve_context")
        self.workflow.add_edge("retrieve_context", "generate_response")
        self.workflow.add_edge("generate_response", "review_response")
        
        # ì¡°ê±´ë¶€ ì—£ì§€ - ì‘ë‹µ í’ˆì§ˆì— ë”°ë¥¸ ë¶„ê¸°
        self.workflow.add_conditional_edges(
            "review_response",
            self.should_escalate_to_human,
            {
                "send_response": "send_response",
                "escalate_to_human": "escalate_to_human"
            }
        )
        
        self.workflow.add_edge("send_response", END)
        self.workflow.add_edge("escalate_to_human", END)

    async def classify_inquiry(self, state: QnAState) -> QnAState:
        """ë¬¸ì˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        classification_prompt = ChatPromptTemplate.from_template("""
        ë‹¤ìŒ ê³ ê° ë¬¸ì˜ë¥¼ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:
        
        ë¬¸ì˜ ë‚´ìš©: {inquiry_text}
        
        ì¹´í…Œê³ ë¦¬ ì˜µì…˜:
        - ì œí’ˆ_ë¬¸ì˜: ì œí’ˆ ì •ë³´, ì‚¬ì–‘, íŠ¹ì§• ê´€ë ¨
        - ê¸°ìˆ _ì§€ì›: ì‹œê³µ, ì‚¬ìš©ë²•, ê¸°ìˆ ì  ë„ì›€
        - ê²¬ì _ìš”ì²­: ê°€ê²©, êµ¬ë§¤, ì£¼ë¬¸ ê´€ë ¨
        - ì¼ë°˜_ë¬¸ì˜: íšŒì‚¬ ì •ë³´, ì—°ë½ì²˜ ë“±
        - ë¶ˆë§Œ_ì²˜ë¦¬: ë¬¸ì œ ì‹ ê³ , ë¶ˆë§Œ ì‚¬í•­
        
        ë¶„ë¥˜ ê²°ê³¼ë§Œ ë°˜í™˜í•˜ì„¸ìš” (ì˜ˆ: ì œí’ˆ_ë¬¸ì˜):
        """)
        
        result = await self.llm.ainvoke(
            classification_prompt.format(inquiry_text=state["inquiry_text"])
        )
        
        state["category"] = result.content.strip()
        return state

    async def assess_urgency(self, state: QnAState) -> QnAState:
        """ê¸´ê¸‰ë„ í‰ê°€"""
        urgency_prompt = ChatPromptTemplate.from_template("""
        ë‹¤ìŒ ë¬¸ì˜ì˜ ê¸´ê¸‰ë„ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”:
        
        ë¬¸ì˜ ë‚´ìš©: {inquiry_text}
        ì¹´í…Œê³ ë¦¬: {category}
        
        ê¸´ê¸‰ë„ ë ˆë²¨:
        - ë†’ìŒ: ì¦‰ì‹œ ëŒ€ì‘ í•„ìš” (ë¶ˆë§Œ, ê¸´ê¸‰ ê¸°ìˆ  ì§€ì›)
        - ë³´í†µ: 24ì‹œê°„ ë‚´ ëŒ€ì‘ (ì¼ë°˜ ê¸°ìˆ  ë¬¸ì˜, ê²¬ì )
        - ë‚®ìŒ: 48ì‹œê°„ ë‚´ ëŒ€ì‘ (ì¼ë°˜ ì •ë³´ ë¬¸ì˜)
        
        ê¸´ê¸‰ë„ë§Œ ë°˜í™˜í•˜ì„¸ìš” (ì˜ˆ: ë³´í†µ):
        """)
        
        result = await self.llm.ainvoke(
            urgency_prompt.format(
                inquiry_text=state["inquiry_text"],
                category=state["category"]
            )
        )
        
        state["urgency_level"] = result.content.strip()
        return state

    async def retrieve_context(self, state: QnAState) -> QnAState:
        """ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œ ê²€ìƒ‰"""
        from database.vector_store import VectorStore
        
        vector_store = VectorStore()
        vector_store.initialize_store()
        
        # ì¹´í…Œê³ ë¦¬ì— ë”°ë¥¸ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
        search_query = self._optimize_search_query(state["inquiry_text"], state["category"])
        
        # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        relevant_docs = vector_store.similarity_search(search_query, k=6)
        state["context_documents"] = [doc.page_content for doc in relevant_docs]
        
        return state

    async def generate_response(self, state: QnAState) -> QnAState:
        """ìë™ ì‘ë‹µ ìƒì„±"""
        response_prompt = ChatPromptTemplate.from_template("""
        ë‹¹ì‹ ì€ ë¦°ì½”ë¦¬ì•„ì˜ ì „ë¬¸ ê³ ê° ìƒë‹´ì›ì…ë‹ˆë‹¤.
        
        ë¬¸ì˜ ì •ë³´:
        - ì¹´í…Œê³ ë¦¬: {category}
        - ê¸´ê¸‰ë„: {urgency_level}
        - ë¬¸ì˜ ë‚´ìš©: {inquiry_text}
        
        ì°¸ê³  ì •ë³´:
        {context}
        
        ë‹¤ìŒ ì§€ì¹¨ì— ë”°ë¼ ì‘ë‹µì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        1. ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ í†¤ ìœ ì§€
        2. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì •ë³´ ì œê³µ
        3. ì¶”ê°€ ë¬¸ì˜ë‚˜ ìƒë‹´ ì±„ë„ ì•ˆë‚´
        4. íšŒì‚¬ì˜ ì „ë¬¸ì„±ê³¼ ì‹ ë¢°ì„± ê°•ì¡°
        
        ì‘ë‹µ:
        """)
        
        context = "\n\n".join(state["context_documents"])
        
        result = await self.llm.ainvoke(
            response_prompt.format(
                category=state["category"],
                urgency_level=state["urgency_level"],
                inquiry_text=state["inquiry_text"],
                context=context
            )
        )
        
        state["generated_response"] = result.content
        return state

    async def review_response(self, state: QnAState) -> QnAState:
        """ì‘ë‹µ í’ˆì§ˆ ê²€í† """
        review_prompt = ChatPromptTemplate.from_template("""
        ë‹¤ìŒ ê³ ê° ì‘ë‹µì˜ í’ˆì§ˆì„ 0-100ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
        
        ì›ë˜ ë¬¸ì˜: {inquiry_text}
        ìƒì„±ëœ ì‘ë‹µ: {response}
        
        í‰ê°€ ê¸°ì¤€:
        - ë¬¸ì˜ ë‚´ìš©ê³¼ì˜ ê´€ë ¨ì„± (30ì )
        - ì •ë³´ì˜ ì •í™•ì„±ê³¼ ì™„ì„±ë„ (30ì )
        - ì‘ë‹µì˜ ëª…í™•ì„±ê³¼ ì´í•´ë„ (20ì )
        - ê³ ê° ì„œë¹„ìŠ¤ í’ˆì§ˆ (20ì )
        
        ì ìˆ˜ë§Œ ìˆ«ìë¡œ ë°˜í™˜í•˜ì„¸ìš” (ì˜ˆ: 85):
        """)
        
        result = await self.llm.ainvoke(
            review_prompt.format(
                inquiry_text=state["inquiry_text"],
                response=state["generated_response"]
            )
        )
        
        try:
            confidence_score = float(result.content.strip())
            state["confidence_score"] = confidence_score
            state["requires_human_review"] = confidence_score < 75
        except:
            state["confidence_score"] = 0
            state["requires_human_review"] = True
        
        return state

    def should_escalate_to_human(self, state: QnAState) -> str:
        """ì¸ê°„ ê°œì… í•„ìš”ì„± íŒë‹¨"""
        if state["requires_human_review"] or state["urgency_level"] == "ë†’ìŒ":
            return "escalate_to_human"
        return "send_response"

    async def send_response(self, state: QnAState) -> QnAState:
        """ìë™ ì‘ë‹µ ë°œì†¡"""
        # ì‹¤ì œ ì‘ë‹µ ë°œì†¡ ë¡œì§ (ì´ë©”ì¼, ì•Œë¦¼ ë“±)
        state["next_action"] = "response_sent"
        return state

    async def escalate_to_human(self, state: QnAState) -> QnAState:
        """ì¸ê°„ ì „ë¬¸ê°€ì—ê²Œ ì—ìŠ¤ì»¬ë ˆì´ì…˜"""
        state["next_action"] = "human_review_required"
        return state

    def _optimize_search_query(self, inquiry_text: str, category: str) -> str:
        """ì¹´í…Œê³ ë¦¬ë³„ ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”"""
        category_keywords = {
            "ì œí’ˆ_ë¬¸ì˜": "ì œí’ˆ ì‚¬ì–‘ íŠ¹ì§•",
            "ê¸°ìˆ _ì§€ì›": "ì‹œê³µ ì„¤ì¹˜ ì‚¬ìš©ë²•",
            "ê²¬ì _ìš”ì²­": "ê°€ê²© ê²¬ì  êµ¬ë§¤",
            "ì¼ë°˜_ë¬¸ì˜": "íšŒì‚¬ ì •ë³´ ì—°ë½ì²˜"
        }
        
        additional_keywords = category_keywords.get(category, "")
        return f"{inquiry_text} {additional_keywords}"

    def compile_workflow(self):
        """ì›Œí¬í”Œë¡œìš° ì»´íŒŒì¼"""
        return self.workflow.compile()
```

### **2. ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ê¸°**

```python
# backend/ai-service/workflows/workflow_executor.py
import asyncio
from typing import Dict, Any
from workflows.qna_workflow import QnAWorkflow, QnAState

class WorkflowExecutor:
    def __init__(self):
        self.qna_workflow = QnAWorkflow()
        self.compiled_workflow = self.qna_workflow.compile_workflow()

    async def process_inquiry(self, inquiry_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë¬¸ì˜ ì²˜ë¦¬ ì‹¤í–‰"""
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state: QnAState = {
            "inquiry_id": inquiry_data["id"],
            "inquiry_text": f"{inquiry_data['title']} {inquiry_data['content']}",
            "user_info": {
                "name": inquiry_data["name"],
                "email": inquiry_data["email"],
                "phone": inquiry_data.get("phone")
            },
            "category": "",
            "urgency_level": "",
            "context_documents": [],
            "generated_response": "",
            "requires_human_review": False,
            "confidence_score": 0.0,
            "next_action": ""
        }

        try:
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            result = await self.compiled_workflow.ainvoke(initial_state)
            
            return {
                "success": True,
                "inquiry_id": result["inquiry_id"],
                "category": result["category"],
                "urgency_level": result["urgency_level"],
                "generated_response": result["generated_response"],
                "confidence_score": result["confidence_score"],
                "requires_human_review": result["requires_human_review"],
                "next_action": result["next_action"]
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "inquiry_id": inquiry_data["id"]
            }

    async def batch_process_inquiries(self, inquiries: list) -> list:
        """ë‹¤ì¤‘ ë¬¸ì˜ ë°°ì¹˜ ì²˜ë¦¬"""
        tasks = [self.process_inquiry(inquiry) for inquiry in inquiries]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                processed_results.append({
                    "success": False,
                    "error": str(result)
                })
            else:
                processed_results.append(result)
        
        return processed_results
```

---

## ğŸ“‹ **Day 12-14: í†µí•© & ìµœì í™”**

### **1. Q&A ì‹œìŠ¤í…œ í†µí•©**

```python
# backend/ai-service/api/qna_endpoints.py
from fastapi import APIRouter, HTTPException, BackgroundTasks
from workflows.workflow_executor import WorkflowExecutor
from database.supabase_client import SupabaseClient
from pydantic import BaseModel
from typing import Optional

router = APIRouter(prefix="/api/qna", tags=["QnA Automation"])

class InquiryProcessRequest(BaseModel):
    inquiry_id: str
    auto_process: bool = True

class BatchProcessRequest(BaseModel):
    inquiry_ids: list[str]
    auto_process: bool = True

workflow_executor = WorkflowExecutor()
supabase_client = SupabaseClient()

@router.post("/process/{inquiry_id}")
async def process_single_inquiry(
    inquiry_id: str,
    background_tasks: BackgroundTasks
):
    """ë‹¨ì¼ ë¬¸ì˜ ìë™ ì²˜ë¦¬"""
    try:
        # ë¬¸ì˜ ë°ì´í„° ì¡°íšŒ
        inquiry_data = await supabase_client.get_inquiry(inquiry_id)
        if not inquiry_data:
            raise HTTPException(status_code=404, detail="ë¬¸ì˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        result = await workflow_executor.process_inquiry(inquiry_data)
        
        # ê²°ê³¼ ì €ì¥
        if result["success"]:
            background_tasks.add_task(
                save_workflow_result,
                inquiry_id,
                result
            )
        
        return result
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/batch-process")
async def batch_process_inquiries(
    request: BatchProcessRequest,
    background_tasks: BackgroundTasks
):
    """ë‹¤ì¤‘ ë¬¸ì˜ ë°°ì¹˜ ì²˜ë¦¬"""
    try:
        # ë¬¸ì˜ ë°ì´í„° ì¡°íšŒ
        inquiries = []
        for inquiry_id in request.inquiry_ids:
            inquiry_data = await supabase_client.get_inquiry(inquiry_id)
            if inquiry_data:
                inquiries.append(inquiry_data)
        
        # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
        results = await workflow_executor.batch_process_inquiries(inquiries)
        
        # ê²°ê³¼ ì €ì¥
        background_tasks.add_task(
            save_batch_results,
            results
        )
        
        return {
            "processed_count": len(results),
            "success_count": sum(1 for r in results if r.get("success")),
            "results": results
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

async def save_workflow_result(inquiry_id: str, result: dict):
    """ì›Œí¬í”Œë¡œìš° ê²°ê³¼ ì €ì¥"""
    try:
        await supabase_client.update_inquiry_with_ai_response(
            inquiry_id,
            {
                "ai_category": result["category"],
                "ai_urgency": result["urgency_level"],
                "ai_response": result["generated_response"],
                "ai_confidence": result["confidence_score"],
                "requires_human_review": result["requires_human_review"],
                "processed_at": "now()"
            }
        )
    except Exception as e:
        print(f"ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")
```

### **2. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ**

```python
# backend/ai-service/monitoring/performance_monitor.py
from datetime import datetime, timedelta
import asyncio
from typing import Dict, List
import logging

class PerformanceMonitor:
    def __init__(self):
        self.metrics = {
            "total_processed": 0,
            "auto_resolved": 0,
            "escalated_to_human": 0,
            "average_confidence": 0.0,
            "processing_time": [],
            "category_distribution": {},
            "hourly_volume": {}
        }
        
    async def log_workflow_execution(self, 
                                   start_time: datetime,
                                   end_time: datetime,
                                   result: Dict):
        """ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ë¡œê·¸"""
        processing_time = (end_time - start_time).total_seconds()
        
        self.metrics["total_processed"] += 1
        self.metrics["processing_time"].append(processing_time)
        
        if result.get("requires_human_review"):
            self.metrics["escalated_to_human"] += 1
        else:
            self.metrics["auto_resolved"] += 1
            
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ ì—…ë°ì´íŠ¸
        category = result.get("category", "unknown")
        self.metrics["category_distribution"][category] = (
            self.metrics["category_distribution"].get(category, 0) + 1
        )
        
        # ì‹œê°„ë³„ ì²˜ë¦¬ëŸ‰ ì—…ë°ì´íŠ¸
        hour_key = start_time.strftime("%Y-%m-%d-%H")
        self.metrics["hourly_volume"][hour_key] = (
            self.metrics["hourly_volume"].get(hour_key, 0) + 1
        )
        
        # í‰ê·  ì‹ ë¢°ë„ ì—…ë°ì´íŠ¸
        confidence = result.get("confidence_score", 0)
        total_confidence = (self.metrics["average_confidence"] * 
                          (self.metrics["total_processed"] - 1) + confidence)
        self.metrics["average_confidence"] = total_confidence / self.metrics["total_processed"]

    def get_performance_report(self) -> Dict:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        if not self.metrics["processing_time"]:
            return {"error": "ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
            
        avg_processing_time = sum(self.metrics["processing_time"]) / len(self.metrics["processing_time"])
        auto_resolution_rate = (self.metrics["auto_resolved"] / 
                              self.metrics["total_processed"] * 100) if self.metrics["total_processed"] > 0 else 0
        
        return {
            "summary": {
                "total_processed": self.metrics["total_processed"],
                "auto_resolved": self.metrics["auto_resolved"],
                "escalated_to_human": self.metrics["escalated_to_human"],
                "auto_resolution_rate": round(auto_resolution_rate, 2),
                "average_confidence": round(self.metrics["average_confidence"], 2),
                "average_processing_time": round(avg_processing_time, 2)
            },
            "category_distribution": self.metrics["category_distribution"],
            "hourly_volume": self.metrics["hourly_volume"]
        }
```

---

## ğŸš€ **Week 2 ì™„ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸**

### **ê¸°ìˆ ì  êµ¬í˜„**
- [ ] LangGraph ìƒíƒœ ë¨¸ì‹  êµ¬ì¶•
- [ ] ë¬¸ì˜ ë¶„ë¥˜ ìë™í™”
- [ ] ê¸´ê¸‰ë„ í‰ê°€ ì‹œìŠ¤í…œ
- [ ] ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ë‹µë³€ ìƒì„±
- [ ] í’ˆì§ˆ ê²€í†  ë° ì—ìŠ¤ì»¬ë ˆì´ì…˜

### **ì›Œí¬í”Œë¡œìš° ìµœì í™”**
- [ ] ì¡°ê±´ë¶€ ë¶„ê¸° ë¡œì§
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ì‹œìŠ¤í…œ
- [ ] ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- [ ] ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µêµ¬

### **í†µí•© ë° í…ŒìŠ¤íŠ¸**
- [ ] ê¸°ì¡´ Q&A ì‹œìŠ¤í…œ ì—°ë™
- [ ] API ì—”ë“œí¬ì¸íŠ¸ êµ¬í˜„
- [ ] ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- [ ] A/B í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶•

**Week 2 ì™„ë£Œ ì‹œ ê²°ê³¼ë¬¼:**
âœ… ì§€ëŠ¥í˜• Q&A ìë™í™” ì‹œìŠ¤í…œ
âœ… LangGraph ì›Œí¬í”Œë¡œìš° ì—”ì§„
âœ… ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
âœ… í™•ì¥ ê°€ëŠ¥í•œ ìƒíƒœ ë¨¸ì‹ 

**Week 2ì—ì„œ ë‹¬ì„±í•˜ëŠ” í•µì‹¬ ê°€ì¹˜:**
ğŸ”¥ **ìë™í™”ìœ¨ 70% ë‹¬ì„±**
ğŸ”¥ **ì‘ë‹µ ì‹œê°„ 80% ë‹¨ì¶•**  
ğŸ”¥ **ì¼ê´€ëœ í’ˆì§ˆì˜ ê³ ê° ì„œë¹„ìŠ¤**
ğŸ”¥ **ì‹¤ì‹œê°„ ì›Œí¬í”Œë¡œìš° ëª¨ë‹ˆí„°ë§**

ì´ì œ Week 3ì—ì„œ ìŠ¤ë§ˆíŠ¸ ê²¬ì  ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ì¤€ë¹„ê°€ ì™„ë£Œë©ë‹ˆë‹¤! ğŸš€ 