# ï¿½ï¿½ Quick Start Guide: Vercelì—ì„œ ì¦‰ì‹œ ì‹œì‘í•˜ê¸°

## âš¡ **6ì›” 20ì¼ ë°”ë¡œ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ì¤€ë¹„ ì‚¬í•­**

### **1. í•„ìˆ˜ ê³„ì • ë° ë„êµ¬ ì¤€ë¹„** (ì†Œìš”ì‹œê°„: 20ë¶„)

```bash
# í•„ìˆ˜ ê³„ì •ë“¤
âœ… Vercel ê³„ì • (https://vercel.com)
âœ… GitHub ê³„ì • (ì½”ë“œ ì €ì¥ìš©)
âœ… ëª¨ë“  API í‚¤ ì¤€ë¹„ ì™„ë£Œ

# ê°œë°œ ë„êµ¬
âœ… Node.js 18+ ì„¤ì¹˜
âœ… Git ì„¤ì¹˜
âœ… VS Code (ë˜ëŠ” ì„ í˜¸í•˜ëŠ” ì—ë””í„°)
```

### **2. API í‚¤ í™•ì¸** (ëª¨ë‘ ì¤€ë¹„ ì™„ë£Œ)
```env
# ë‹¤ì¤‘ LLM ì§€ì› (3ê°œ ì œê³µì ëª¨ë‘ í™œì„±í™”)
OPENAI_API_KEY=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
ANTHROPIC_API_KEY=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
GOOGLE_API_KEY=AIzaSyBd3vM8XgfrBWZq3J2upUvIwDsIahdf3lE

# ë°ì´í„°ë² ì´ìŠ¤ (ê¸°ì¡´ í™œìš©)
SUPABASE_URL=https://fpyqjvnpduwifxgnbrck.supabase.co
SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZweXFqdm5wZHV3aWZ4Z25icmNrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDk1NDA0MDQsImV4cCI6MjA2NTExNjQwNH0.Uqn7ajeRHEqMl7gbPtOYHKd1ZhUb6xiMTZsK4QMxtfU
```

---

## ğŸ“‹ **ì²«ì§¸ ë‚  (6/20) ì¦‰ì‹œ ì‹¤í–‰ ê°€ì´ë“œ**

### **Step 1: Next.js í”„ë¡œì íŠ¸ ìƒì„±** (5ë¶„)

```bash
# ìƒˆë¡œìš´ Next.js í”„ë¡œì íŠ¸ ìƒì„±
npx create-next-app@latest rinkorea-ai --typescript --tailwind --app --use-npm
cd rinkorea-ai

# Vercel CLI ì„¤ì¹˜ (ê¸€ë¡œë²Œ)
npm i -g vercel

# AI ê´€ë ¨ íŒ¨í‚¤ì§€ ì„¤ì¹˜
npm install openai @anthropic-ai/sdk @google/generative-ai
npm install langchain @langchain/openai @langchain/anthropic @langchain/community
npm install @supabase/supabase-js
npm install @radix-ui/react-icons lucide-react
```

### **Step 2: í™˜ê²½ë³€ìˆ˜ ì„¤ì •** (2ë¶„)

```bash
# .env.local íŒŒì¼ ìƒì„±
echo "OPENAI_API_KEY=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
ANTHROPIC_API_KEY=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
GOOGLE_API_KEY=AIzaSyBd3vM8XgfrBWZq3J2upUvIwDsIahdf3lE
SUPABASE_URL=https://fpyqjvnpduwifxgnbrck.supabase.co
SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZweXFqdm5wZHV3aWZ4Z25icmNrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDk1NDA0MDQsImV4cCI6MjA2NTExNjQwNH0.Uqn7ajeRHEqMl7gbPtOYHKd1ZhUb6xiMTZsK4QMxtfU" > .env.local
```

### **Step 3: ë‹¤ì¤‘ LLM API Routes êµ¬í˜„** (30ë¶„)

```typescript
// app/api/chat/[provider]/route.ts
import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';
import { GoogleGenerativeAI } from '@google/generative-ai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const googleAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY!);

export async function POST(
  request: NextRequest,
  { params }: { params: { provider: string } }
) {
  try {
    const { message } = await request.json();
    const { provider } = params;

    let response;
    let providerUsed = provider;

    switch (provider) {
      case 'openai':
        const completion = await openai.chat.completions.create({
          model: "gpt-4",
          messages: [
            {
              role: "system",
              content: "ë‹¹ì‹ ì€ ë¦°ì½”ë¦¬ì•„ì˜ AI ìƒë‹´ì›ì…ë‹ˆë‹¤. ì¹œê·¼í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."
            },
            { role: "user", content: message }
          ],
          max_tokens: 1000,
        });
        response = completion.choices[0].message.content;
        break;
        
      case 'anthropic':
        const claudeResponse = await anthropic.messages.create({
          model: "claude-3-5-sonnet-20241022",
          max_tokens: 1000,
          messages: [{ role: "user", content: message }],
        });
        response = claudeResponse.content[0].text;
        break;
        
      case 'google':
        const model = googleAI.getGenerativeModel({ model: "gemini-pro" });
        const result = await model.generateContent(message);
        response = result.response.text();
        break;
        
      default:
        // í´ë°±: OpenAI ì‚¬ìš©
        const fallbackCompletion = await openai.chat.completions.create({
          model: "gpt-4",
          messages: [{ role: "user", content: message }],
          max_tokens: 1000,
        });
        response = fallbackCompletion.choices[0].message.content;
        providerUsed = 'openai-fallback';
    }

    return NextResponse.json({ 
      success: true, 
      response, 
      provider: providerUsed,
      timestamp: new Date().toISOString()
    });

  } catch (error) {
    console.error('Chat API error:', error);
    return NextResponse.json(
      { success: false, error: 'AI ì„œë¹„ìŠ¤ ì¼ì‹œ ì˜¤ë¥˜' },
      { status: 500 }
    );
  }
}
```

### **Step 4: SSE ìŠ¤íŠ¸ë¦¬ë° API êµ¬í˜„** (20ë¶„)

```typescript
// app/api/chat/stream/route.ts
import { NextRequest } from 'next/server';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export async function POST(request: NextRequest) {
  const { message, provider = 'openai' } = await request.json();

  const stream = new ReadableStream({
    async start(controller) {
      try {
        const completion = await openai.chat.completions.create({
          model: "gpt-4",
          messages: [
            {
              role: "system", 
              content: "ë‹¹ì‹ ì€ ë¦°ì½”ë¦¬ì•„ì˜ AI ìƒë‹´ì›ì…ë‹ˆë‹¤."
            },
            { role: "user", content: message }
          ],
          stream: true,
          max_tokens: 1000,
        });

        for await (const chunk of completion) {
          const content = chunk.choices[0]?.delta?.content || '';
          if (content) {
            const data = JSON.stringify({ content, done: false });
            controller.enqueue(`data: ${data}\n\n`);
          }
        }

        controller.enqueue(`data: ${JSON.stringify({ content: '', done: true })}\n\n`);
        controller.close();

      } catch (error) {
        const errorData = JSON.stringify({ error: error.message, done: true });
        controller.enqueue(`data: ${errorData}\n\n`);
        controller.close();
      }
    },
  });

  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
    },
  });
}
```

### **Step 5: ë‹¤ì¤‘ LLM ì±„íŒ… ì»´í¬ë„ŒíŠ¸** (40ë¶„)

```typescript
// components/MultiLLMChat.tsx
'use client';

import { useState, useEffect } from 'react';
import { Send, Bot, User, Sparkles } from 'lucide-react';

interface Message {
  id: string;
  content: string;
  type: 'user' | 'ai' | 'system';
  provider?: string;
  timestamp: Date;
}

const providers = [
  { id: 'openai', name: 'OpenAI GPT-4', icon: 'ğŸ¤–', color: 'bg-green-500' },
  { id: 'anthropic', name: 'Claude 3.5', icon: 'ğŸ§ ', color: 'bg-purple-500' },
  { id: 'google', name: 'Gemini Pro', icon: 'âœ¨', color: 'bg-blue-500' }
];

export default function MultiLLMChat() {
  const [messages, setMessages] = useState<Message[]>([]);
  const [input, setInput] = useState('');
  const [selectedProvider, setSelectedProvider] = useState('openai');
  const [isLoading, setIsLoading] = useState(false);
  const [isStreaming, setIsStreaming] = useState(false);

  useEffect(() => {
    setMessages([{
      id: '1',
      content: 'ì•ˆë…•í•˜ì„¸ìš”! ë¦°ì½”ë¦¬ì•„ AI ìƒë‹´ì›ì…ë‹ˆë‹¤. ì œí’ˆì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”.',
      type: 'system',
      timestamp: new Date()
    }]);
  }, []);

  const sendMessage = async () => {
    if (!input.trim() || isLoading) return;

    const userMessage: Message = {
      id: Date.now().toString(),
      content: input,
      type: 'user',
      timestamp: new Date()
    };

    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsLoading(true);

    try {
      // ì¼ë°˜ ì‘ë‹µ ë°©ì‹
      const response = await fetch(`/api/chat/${selectedProvider}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: input }),
      });

      const data = await response.json();

      if (data.success) {
        const aiMessage: Message = {
          id: (Date.now() + 1).toString(),
          content: data.response,
          type: 'ai',
          provider: data.provider,
          timestamp: new Date()
        };
        setMessages(prev => [...prev, aiMessage]);
      } else {
        throw new Error(data.error);
      }

    } catch (error) {
      const errorMessage: Message = {
        id: (Date.now() + 1).toString(),
        content: 'ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
        type: 'ai',
        timestamp: new Date()
      };
      setMessages(prev => [...prev, errorMessage]);
    } finally {
      setIsLoading(false);
    }
  };

  const sendStreamingMessage = async () => {
    if (!input.trim() || isStreaming) return;

    const userMessage: Message = {
      id: Date.now().toString(),
      content: input,
      type: 'user',
      timestamp: new Date()
    };

    setMessages(prev => [...prev, userMessage]);
    const messageInput = input;
    setInput('');
    setIsStreaming(true);

    try {
      const response = await fetch('/api/chat/stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: messageInput, provider: selectedProvider }),
      });

      const reader = response.body?.getReader();
      let aiMessage: Message = {
        id: (Date.now() + 1).toString(),
        content: '',
        type: 'ai',
        provider: selectedProvider,
        timestamp: new Date()
      };

      setMessages(prev => [...prev, aiMessage]);

      if (reader) {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          const chunk = new TextDecoder().decode(value);
          const lines = chunk.split('\n');
          
          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = JSON.parse(line.slice(6));
              if (data.content) {
                setMessages(prev => {
                  const newMessages = [...prev];
                  const lastMessage = newMessages[newMessages.length - 1];
                  lastMessage.content += data.content;
                  return newMessages;
                });
              }
              if (data.done) break;
            }
          }
        }
      }
    } catch (error) {
      console.error('Streaming error:', error);
    } finally {
      setIsStreaming(false);
    }
  };

  const currentProvider = providers.find(p => p.id === selectedProvider);

  return (
    <div className="fixed bottom-4 right-4 w-96 h-[600px] bg-white border rounded-lg shadow-2xl flex flex-col">
      {/* í—¤ë” - LLM ì„ íƒ */}
      <div className="p-4 border-b bg-gray-50 rounded-t-lg">
        <div className="flex items-center justify-between mb-3">
          <h3 className="font-bold text-lg flex items-center gap-2">
            <Sparkles className="w-5 h-5 text-blue-500" />
            AI ì–´ì‹œìŠ¤í„´íŠ¸
          </h3>
          <div className={`w-3 h-3 rounded-full ${currentProvider?.color}`} />
        </div>
        
        <div className="grid grid-cols-3 gap-1">
          {providers.map((provider) => (
            <button
              key={provider.id}
              onClick={() => setSelectedProvider(provider.id)}
              className={`p-2 text-xs rounded transition-all ${
                selectedProvider === provider.id
                  ? 'bg-blue-500 text-white'
                  : 'bg-white text-gray-700 hover:bg-gray-100'
              }`}
            >
              <div className="text-center">
                <div className="text-lg mb-1">{provider.icon}</div>
                <div className="font-medium">{provider.name.split(' ')[0]}</div>
              </div>
            </button>
          ))}
        </div>
      </div>

      {/* ë©”ì‹œì§€ ì˜ì—­ */}
      <div className="flex-1 overflow-y-auto p-4 space-y-3">
        {messages.map((message) => (
          <div
            key={message.id}
            className={`flex ${message.type === 'user' ? 'justify-end' : 'justify-start'}`}
          >
            <div
              className={`max-w-[85%] rounded-lg p-3 ${
                message.type === 'user'
                  ? 'bg-blue-500 text-white'
                  : message.type === 'system'
                  ? 'bg-green-50 text-green-800 border border-green-200'
                  : 'bg-gray-100 text-gray-800'
              }`}
            >
              <div className="flex items-start gap-2">
                {message.type === 'ai' && <Bot className="w-4 h-4 mt-1 text-blue-500" />}
                {message.type === 'user' && <User className="w-4 h-4 mt-1" />}
                {message.type === 'system' && <Sparkles className="w-4 h-4 mt-1 text-green-600" />}
                <div className="flex-1">
                  <div className="text-sm whitespace-pre-wrap">{message.content}</div>
                  {message.provider && message.type === 'ai' && (
                    <div className="text-xs opacity-70 mt-1">
                      via {message.provider}
                    </div>
                  )}
                </div>
              </div>
            </div>
          </div>
        ))}
        
        {(isLoading || isStreaming) && (
          <div className="flex justify-start">
            <div className="bg-gray-100 rounded-lg p-3">
              <div className="flex items-center gap-2">
                <Bot className="w-4 h-4 text-blue-500" />
                <span className="text-sm text-gray-600">AIê°€ ìƒê°í•˜ê³  ìˆì–´ìš”...</span>
                <div className="flex gap-1">
                  <div className="w-1 h-1 bg-blue-500 rounded-full animate-bounce" />
                  <div className="w-1 h-1 bg-blue-500 rounded-full animate-bounce" style={{ animationDelay: '0.1s' }} />
                  <div className="w-1 h-1 bg-blue-500 rounded-full animate-bounce" style={{ animationDelay: '0.2s' }} />
                </div>
              </div>
            </div>
          </div>
        )}
      </div>

      {/* ì…ë ¥ ì˜ì—­ */}
      <div className="p-4 border-t">
        <div className="flex gap-2">
          <input
            value={input}
            onChange={(e) => setInput(e.target.value)}
            onKeyPress={(e) => e.key === 'Enter' && sendMessage()}
            className="flex-1 border rounded-lg px-3 py-2 text-sm focus:outline-none focus:ring-2 focus:ring-blue-500"
            placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
            disabled={isLoading || isStreaming}
          />
          <button 
            onClick={sendMessage}
            disabled={isLoading || isStreaming || !input.trim()}
            className="bg-blue-500 text-white p-2 rounded-lg hover:bg-blue-600 disabled:opacity-50 disabled:cursor-not-allowed transition-colors"
          >
            <Send className="w-4 h-4" />
          </button>
        </div>
        
        <div className="flex justify-between items-center mt-2 text-xs text-gray-500">
          <span>í˜„ì¬ ì‚¬ìš© ì¤‘: {currentProvider?.name}</span>
          <button
            onClick={sendStreamingMessage}
            disabled={isStreaming || !input.trim()}
            className="text-blue-500 hover:underline disabled:opacity-50"
          >
            ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
          </button>
        </div>
      </div>
    </div>
  );
}
```

### **Step 6: ë©”ì¸ í˜ì´ì§€ì— ì¶”ê°€** (5ë¶„)

```typescript
// app/page.tsx
import MultiLLMChat from '@/components/MultiLLMChat';

export default function Home() {
  return (
    <main className="min-h-screen bg-gray-50">
      <div className="container mx-auto px-4 py-8">
        <h1 className="text-4xl font-bold text-center mb-8">
          RinKorea AI Platform
        </h1>
        <div className="text-center text-gray-600 mb-8">
          <p>ë‹¤ì¤‘ LLMì„ í™œìš©í•œ ì§€ëŠ¥í˜• ìƒë‹´ ì‹œìŠ¤í…œ</p>
          <p className="text-sm mt-2">
            OpenAI GPT-4 â€¢ Anthropic Claude â€¢ Google Gemini
          </p>
        </div>
      </div>
      
      <MultiLLMChat />
    </main>
  );
}
```

---

## ğŸš€ **ì¦‰ì‹œ ì‹¤í–‰ ë° ë°°í¬**

### **Step 7: ë¡œì»¬ í…ŒìŠ¤íŠ¸** (5ë¶„)
```bash
# ê°œë°œ ì„œë²„ ì‹¤í–‰
npm run dev

# ë¸Œë¼ìš°ì €ì—ì„œ í™•ì¸
# http://localhost:3000
# ìš°ì¸¡ í•˜ë‹¨ ì±„íŒ…ì°½ì—ì„œ ë‹¤ì¤‘ LLM í…ŒìŠ¤íŠ¸
```

### **Step 8: Vercel ë°°í¬** (5ë¶„)
```bash
# Vercel ë¡œê·¸ì¸ ë° ë°°í¬
vercel login
vercel --prod

# í™˜ê²½ë³€ìˆ˜ ì¶”ê°€
vercel env add OPENAI_API_KEY
vercel env add ANTHROPIC_API_KEY  
vercel env add GOOGLE_API_KEY
vercel env add SUPABASE_URL
vercel env add SUPABASE_ANON_KEY

# ì¬ë°°í¬
vercel --prod
```

---

## ğŸ“Š **ì²«ë‚  ë‹¬ì„± ëª©í‘œ ì²´í¬ë¦¬ìŠ¤íŠ¸**

### **ì˜¤ì „ (9:00-12:00): ê¸°ë³¸ ì¸í”„ë¼**
- [ ] Next.js í”„ë¡œì íŠ¸ ìƒì„±
- [ ] ë‹¤ì¤‘ LLM API Routes êµ¬í˜„
- [ ] í™˜ê²½ë³€ìˆ˜ ì„¤ì •
- [ ] ê¸°ë³¸ ì±„íŒ… API í…ŒìŠ¤íŠ¸

### **ì˜¤í›„ (1:00-6:00): ì±„íŒ… ì‹œìŠ¤í…œ**
- [ ] SSE ìŠ¤íŠ¸ë¦¬ë° API êµ¬í˜„
- [ ] React ì±„íŒ… ì»´í¬ë„ŒíŠ¸ ì™„ì„±
- [ ] 3ê°œ LLM ì œê³µì ì—°ë™
- [ ] Vercel ë°°í¬ ì™„ë£Œ

**ğŸ¯ ë‹¹ì¼ ë°¤ ì™„ì„± ëª©í‘œ**: **ai.rinkorea.com**ì—ì„œ 3ê°œ LLMìœ¼ë¡œ ì‹¤ì‹œê°„ ì±„íŒ… ê°€ëŠ¥

---

## ğŸ¯ **ì£¼ì°¨ë³„ í•µì‹¬ ëª©í‘œ (Vercel ìµœì í™”)**

### **Week 1 (6/20-6/26): Next.js AI ì±—ë´‡**
```
Day 1-2: Next.js + ë‹¤ì¤‘ LLM API Routes
Day 3-4: LangChain.js RAG ì‹œìŠ¤í…œ  
Day 5-7: SSE ìŠ¤íŠ¸ë¦¬ë° + ìµœì í™”
```

### **Week 2 (6/27-7/3): Edge Functions ì›Œí¬í”Œë¡œìš°**
```
Day 8-9: JavaScript ì›Œí¬í”Œë¡œìš° ìƒíƒœ ë¨¸ì‹ 
Day 10-11: API Routes Q&A ìë™í™”
Day 12-14: Supabase ì—°ë™ + ëª¨ë‹ˆí„°ë§
```

### **Week 3 (7/4-7/10): ê²¬ì  ì‹œìŠ¤í…œ**
```
Day 15-16: AI ê²¬ì  ìƒì„± API
Day 17-18: í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ PDF ìƒì„±
Day 19-21: ì „ì²´ ì‹œìŠ¤í…œ í†µí•©
```

### **Week 4 (7/11-7/13): ì™„ì„± & ë°°í¬**
```
Day 22: ë¸Œë¼ìš°ì € ê¸°ë°˜ ë¬¸ì„œ ì²˜ë¦¬
Day 23: Vercel ìµœì í™” + ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
Day 24: í¬íŠ¸í´ë¦¬ì˜¤ ì™„ì„±
```

---

## ğŸ†˜ **Vercel íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ**

### **API Routes 500 ì—ëŸ¬**
```bash
# Vercel Function ë¡œê·¸ í™•ì¸
vercel logs

# í™˜ê²½ë³€ìˆ˜ í™•ì¸
vercel env ls
```

### **í™˜ê²½ë³€ìˆ˜ ë¯¸ì¸ì‹**
```typescript
// í™˜ê²½ë³€ìˆ˜ ê²€ì¦ ì½”ë“œ ì¶”ê°€
if (!process.env.OPENAI_API_KEY) {
  throw new Error('OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤');
}
```

### **CORS ì—ëŸ¬**
```typescript
// API Routeì—ì„œ CORS í—¤ë” ì¶”ê°€
export async function POST(request: NextRequest) {
  const response = NextResponse.json(data);
  response.headers.set('Access-Control-Allow-Origin', '*');
  return response;
}
```

### **Vercel Function íƒ€ì„ì•„ì›ƒ**
```typescript
// ìš”ì²­ íƒ€ì„ì•„ì›ƒ ì„¤ì •
const timeoutPromise = new Promise((_, reject) => 
  setTimeout(() => reject(new Error('timeout')), 55000) // 55ì´ˆ
);

const result = await Promise.race([aiRequest, timeoutPromise]);
```

---

## ğŸ’¡ **Vercel ì„±ê³µ íŒ**

### **1. ê°œë°œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”**
- í•« ë¦¬ë¡œë“œë¡œ ì¦‰ì‹œ í…ŒìŠ¤íŠ¸
- Vercel CLIë¡œ ì‹¤ì‹œê°„ ë¡œê·¸ í™•ì¸
- í™˜ê²½ë³€ìˆ˜ëŠ” ì›¹ ëŒ€ì‹œë³´ë“œì—ì„œ ê´€ë¦¬

### **2. ì„±ëŠ¥ ìµœì í™”**
- API RoutesëŠ” 60ì´ˆ ì œí•œ í™œìš©
- í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ ìºì‹± ì ê·¹ í™œìš©
- SSEë¡œ ì‚¬ìš©ì ê²½í—˜ ê°œì„ 

### **3. í¬íŠ¸í´ë¦¬ì˜¤ ì°¨ë³„í™”**
- **ë‹¤ì¤‘ LLM ì§€ëŠ¥í˜• ì„ íƒ**: ì‘ì—…ë³„ ìµœì  AI ì‚¬ìš©
- **Vercel ìµœì‹  ê¸°ìˆ **: Edge Functions + SSE ìŠ¤íŠ¸ë¦¬ë°
- **ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì—°ë™**: ìš´ì˜ ì¤‘ì¸ rinkorea.com í™œìš©

---

## ğŸ‰ **ìµœì¢… ëª©í‘œ (7ì›” 13ì¼)**

**ì™„ì„±ë  ai.rinkorea.com ì‚¬ì´íŠ¸:**

âœ… **ë‹¤ì¤‘ LLM ì±„íŒ… ì‹œìŠ¤í…œ** (3ê°œ ì œê³µì)  
âœ… **Edge Functions ì›Œí¬í”Œë¡œìš°** (Q&A ìë™í™”)  
âœ… **AI ê²¬ì  ìƒì„± ì‹œìŠ¤í…œ** (PDF ì¶œë ¥)  
âœ… **ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§** (Vercel Analytics)  
âœ… **ì™„ì„±ë„ ë†’ì€ UX/UI** (ëª¨ë°”ì¼ ìµœì í™”)  

---

## ğŸš€ **ì¦‰ì‹œ ì‹œì‘ ëª…ë ¹ì–´**

```bash
# 6ì›” 20ì¼ ì˜¤ì „ 9ì‹œë¶€í„° ë°”ë¡œ ì‹¤í–‰
npx create-next-app@latest rinkorea-ai --typescript --tailwind --app --use-npm
cd rinkorea-ai
npm install openai @anthropic-ai/sdk @google/generative-ai langchain @langchain/openai @langchain/anthropic @supabase/supabase-js
echo "OPENAI_API_KEY=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" > .env.local
npm run dev
```

**ì²«ë‚  ì €ë…ê¹Œì§€**: 3ê°œ LLMìœ¼ë¡œ ì‹¤ì‹œê°„ ì±„íŒ…í•˜ëŠ” ai.rinkorea.com ì™„ì„±! ğŸ¯

**MIDAS í¬íŠ¸í´ë¦¬ì˜¤**: **Vercel + Next.js 14 + ë‹¤ì¤‘ LLM** ìµœì‹  ê¸°ìˆ  ìŠ¤íƒ ì™„ì „ ë§ˆìŠ¤í„°! ğŸš€

í™”ì´íŒ…! ğŸ’ªâœ¨