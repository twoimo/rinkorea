# ğŸ¤– ë‹¤ì¤‘ LLM ì„¤ì • ê°€ì´ë“œ

## ğŸ¯ **MIDAS í¬íŠ¸í´ë¦¬ì˜¤ í•µì‹¬ ì°¨ë³„í™” ìš”ì†Œ**

**3ê°œ ì£¼ìš” LLM APIë¥¼ ëª¨ë‘ ì§€ì›í•˜ëŠ” ìœ ì—°í•œ AI ì‹œìŠ¤í…œ**ìœ¼ë¡œ **ìµœê³  ìˆ˜ì¤€ì˜ AI/LLM ê°œë°œ ì—­ëŸ‰**ì„ ì¦ëª…í•©ë‹ˆë‹¤!

---

## ğŸ” **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**

### **.env íŒŒì¼ êµ¬ì„±**
```bash
# backend/ai-service/.env

# === ë‹¤ì¤‘ LLM API í‚¤ ===
# OpenAI GPT-4 (ê°€ì¥ ë²”ìš©ì )
OPENAI_API_KEY=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

# Anthropic Claude (ì¶”ë¡  ëŠ¥ë ¥ ìš°ìˆ˜)
ANTHROPIC_API_KEY=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

# Google Gemini (ë‹¤êµ­ì–´ ì§€ì› ê°•ë ¥)
GOOGLE_API_KEY=AIzaSyBd3vM8XgfrBWZq3J2upUvIwDsIahdf3lE

# === ê¸°ë³¸ ì„¤ì • ===
DEFAULT_LLM_PROVIDER=openai
FALLBACK_LLM_PROVIDER=anthropic

# === Database & Vector DB ì„¤ì • ===
SUPABASE_URL=https://fpyqjvnpduwifxgnbrck.supabase.co
SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZweXFqdm5wZHV3aWZ4Z25icmNrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDk1NDA0MDQsImV4cCI6MjA2NTExNjQwNH0.Uqn7ajeRHEqMl7gbPtOYHKd1ZhUb6xiMTZsK4QMxtfU
PINECONE_API_KEY=pcsk_4AQeoW_T5yG4j6JcG2kbvQZjTsrBg5HzyYofmLZUixjrCz3gG4s4sBWVN6TZ819BzNzCY8
```

---

## ğŸ”§ **ë‹¤ì¤‘ LLM íŒ©í† ë¦¬ êµ¬í˜„**

### **1. LLM íŒ©í† ë¦¬ í´ë˜ìŠ¤**
```python
# backend/ai-service/llm/llm_factory.py
from typing import Optional
from enum import Enum
import os
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
from dotenv import load_dotenv

load_dotenv()

class LLMProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"

class LLMFactory:
    """ë‹¤ì¤‘ LLM ì œê³µìë¥¼ ê´€ë¦¬í•˜ëŠ” íŒ©í† ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.api_keys = {
            LLMProvider.OPENAI: os.getenv("OPENAI_API_KEY"),
            LLMProvider.ANTHROPIC: os.getenv("ANTHROPIC_API_KEY"),
            LLMProvider.GOOGLE: os.getenv("GOOGLE_API_KEY"),
        }
        
        self.model_configs = {
            LLMProvider.OPENAI: {
                "models": ["gpt-4", "gpt-3.5-turbo"],
                "default": "gpt-4"
            },
            LLMProvider.ANTHROPIC: {
                "models": ["claude-3-5-sonnet-20241022", "claude-3-haiku-20240307"],
                "default": "claude-3-5-sonnet-20241022"
            },
            LLMProvider.GOOGLE: {
                "models": ["gemini-pro"],
                "default": "gemini-pro"
            }
        }
    
    def create_llm(self, 
                   provider: LLMProvider,
                   model: Optional[str] = None,
                   temperature: float = 0.7,
                   max_tokens: int = 4000):
        """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        
        if not self.api_keys[provider]:
            raise ValueError(f"{provider.value} API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        model = model or self.model_configs[provider]["default"]
        
        if provider == LLMProvider.OPENAI:
            return ChatOpenAI(
                api_key=self.api_keys[provider],
                model=model,
                temperature=temperature,
                max_tokens=max_tokens
            )
        
        elif provider == LLMProvider.ANTHROPIC:
            return ChatAnthropic(
                api_key=self.api_keys[provider],
                model=model,
                temperature=temperature,
                max_tokens=max_tokens
            )
        
        elif provider == LLMProvider.GOOGLE:
            return ChatGoogleGenerativeAI(
                google_api_key=self.api_keys[provider],
                model=model,
                temperature=temperature,
                max_output_tokens=max_tokens
            )
    
    def get_available_providers(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì œê³µì ëª©ë¡"""
        return [
            provider for provider, api_key in self.api_keys.items()
            if api_key is not None
        ]
    
    def test_provider(self, provider: LLMProvider):
        """ì œê³µì ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            llm = self.create_llm(provider)
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€
            response = llm.invoke("Hello")
            return True, response.content
        except Exception as e:
            return False, str(e)

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
llm_factory = LLMFactory()
```

### **2. ì§€ëŠ¥í˜• LLM ë§¤ë‹ˆì €**
```python
# backend/ai-service/llm/llm_manager.py
import asyncio
from datetime import datetime
from llm.llm_factory import LLMFactory, LLMProvider, llm_factory
import logging

logger = logging.getLogger(__name__)

class SmartLLMManager:
    """ì§€ëŠ¥í˜• LLM ì„ íƒ ë° ê´€ë¦¬"""
    
    def __init__(self):
        self.factory = llm_factory
        self.performance_stats = {}
        self.fallback_order = [
            LLMProvider.OPENAI,
            LLMProvider.ANTHROPIC,
            LLMProvider.GOOGLE
        ]
    
    async def get_best_llm(self, task_type: str = "general"):
        """ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ ìµœì  LLM ì„ íƒ"""
        
        # ì‘ì—… ìœ í˜•ë³„ ìµœì  ì œê³µì
        task_preferences = {
            "coding": LLMProvider.OPENAI,  # ì½”ë”©ì€ GPT-4ê°€ ìš°ìˆ˜
            "reasoning": LLMProvider.ANTHROPIC,  # ì¶”ë¡ ì€ Claudeê°€ ê°•ë ¥
            "multilingual": LLMProvider.GOOGLE,  # ë‹¤êµ­ì–´ëŠ” Gemini
            "general": LLMProvider.OPENAI  # ê¸°ë³¸ê°’
        }
        
        preferred_provider = task_preferences.get(task_type, LLMProvider.OPENAI)
        
        # ì„±ëŠ¥ ê¸°ë°˜ í´ë°± ì²˜ë¦¬
        for provider in [preferred_provider] + self.fallback_order:
            try:
                llm = self.factory.create_llm(provider)
                await self._test_llm_connection(llm, provider)
                
                logger.info(f"ì„ íƒëœ LLM: {provider.value} (ì‘ì—…: {task_type})")
                return llm, provider
                
            except Exception as e:
                logger.warning(f"LLM ì‹¤íŒ¨ {provider.value}: {e}")
                continue
        
        raise Exception("ëª¨ë“  LLM ì œê³µìì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    async def _test_llm_connection(self, llm, provider: LLMProvider):
        """LLM ì—°ê²° ë° ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        start_time = datetime.now()
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ìš”ì²­
        response = await llm.ainvoke("ì•ˆë…•í•˜ì„¸ìš”")
        
        response_time = (datetime.now() - start_time).total_seconds()
        
        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        if provider.value not in self.performance_stats:
            self.performance_stats[provider.value] = {
                "total_requests": 0,
                "success_count": 0,
                "avg_response_time": 0,
                "last_success": None
            }
        
        stats = self.performance_stats[provider.value]
        stats["total_requests"] += 1
        stats["success_count"] += 1
        stats["last_success"] = datetime.now()
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
        if stats["avg_response_time"] == 0:
            stats["avg_response_time"] = response_time
        else:
            stats["avg_response_time"] = (stats["avg_response_time"] + response_time) / 2
    
    def get_performance_report(self):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        return {
            "provider_stats": self.performance_stats,
            "recommendation": self._get_best_performer()
        }
    
    def _get_best_performer(self):
        """ê°€ì¥ ì„±ëŠ¥ì´ ì¢‹ì€ ì œê³µì ì¶”ì²œ"""
        best_provider = None
        best_score = 0
        
        for provider_name, stats in self.performance_stats.items():
            if stats["total_requests"] == 0:
                continue
            
            success_rate = stats["success_count"] / stats["total_requests"]
            speed_score = 1 / (stats["avg_response_time"] + 0.1)
            
            # ì¢…í•© ì ìˆ˜ (ì„±ê³µë¥  70% + ì†ë„ 30%)
            total_score = success_rate * 0.7 + (speed_score / 10) * 0.3
            
            if total_score > best_score:
                best_score = total_score
                best_provider = provider_name
        
        return best_provider

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
smart_llm_manager = SmartLLMManager()
```

---

## ğŸ¨ **ì›¹ ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„**

### **FastAPI ì—”ë“œí¬ì¸íŠ¸**
```python
# backend/ai-service/api/multi_llm_endpoints.py
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from llm.llm_manager import smart_llm_manager
from llm.llm_factory import LLMProvider
from typing import Optional

router = APIRouter(prefix="/api/multi-llm", tags=["Multi-LLM System"])

class ChatRequest(BaseModel):
    message: str
    provider: Optional[str] = None
    task_type: Optional[str] = "general"
    model: Optional[str] = None

@router.get("/providers")
async def get_available_providers():
    """ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì ë° ëª¨ë¸ ëª©ë¡"""
    return {
        "providers": [
            {
                "value": "openai",
                "label": "OpenAI GPT-4",
                "icon": "ğŸ¤–",
                "models": ["gpt-4", "gpt-3.5-turbo"],
                "strengths": ["ì½”ë”©", "ë²”ìš©ì  ì‚¬ìš©", "ë¹ ë¥¸ ì‘ë‹µ"]
            },
            {
                "value": "anthropic",
                "label": "Claude 3.5 Sonnet",
                "icon": "ğŸ§ ",
                "models": ["claude-3-5-sonnet-20241022", "claude-3-haiku-20240307"],
                "strengths": ["ì¶”ë¡ ", "ë¶„ì„", "ê¸´ ë¬¸ë§¥ ì´í•´"]
            },
            {
                "value": "google",
                "label": "Google Gemini Pro",
                "icon": "âœ¨",
                "models": ["gemini-pro"],
                "strengths": ["ë‹¤êµ­ì–´", "ì°½ì˜ì  ì‘ì—…", "ì´ë¯¸ì§€ ë¶„ì„"]
            }
        ]
    }

@router.post("/chat")
async def chat_with_multi_llm(request: ChatRequest):
    """ë‹¤ì¤‘ LLM ì§€ì› ì±„íŒ…"""
    try:
        if request.provider:
            # íŠ¹ì • ì œê³µì ì§€ì •
            provider = LLMProvider(request.provider)
            llm = smart_llm_manager.factory.create_llm(
                provider=provider,
                model=request.model
            )
            used_provider = provider
        else:
            # ì§€ëŠ¥í˜• ì„ íƒ
            llm, used_provider = await smart_llm_manager.get_best_llm(request.task_type)
        
        # ì‘ë‹µ ìƒì„±
        response = await llm.ainvoke(request.message)
        
        return {
            "success": True,
            "response": response.content,
            "provider_used": used_provider.value,
            "task_type": request.task_type,
            "model_used": request.model or "default"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì±„íŒ… ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")

@router.get("/performance")
async def get_performance_stats():
    """LLM ì„±ëŠ¥ í†µê³„"""
    return smart_llm_manager.get_performance_report()

@router.post("/test/{provider}")
async def test_llm_provider(provider: str):
    """íŠ¹ì • LLM ì œê³µì í…ŒìŠ¤íŠ¸"""
    try:
        llm_provider = LLMProvider(provider)
        success, result = smart_llm_manager.factory.test_provider(llm_provider)
        
        return {
            "provider": provider,
            "success": success,
            "result": result,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
```

---

## ğŸ¨ **React ì»´í¬ë„ŒíŠ¸**

### **LLM ì„ íƒê¸° ì»´í¬ë„ŒíŠ¸**
```typescript
// src/components/ai/LLMProviderSelector.tsx
import React, { useState, useEffect } from 'react';

interface Provider {
  value: string;
  label: string;
  icon: string;
  models: string[];
  strengths: string[];
}

interface LLMProviderSelectorProps {
  onProviderChange: (provider: string, model?: string) => void;
  selectedProvider: string;
}

const LLMProviderSelector: React.FC<LLMProviderSelectorProps> = ({
  onProviderChange,
  selectedProvider
}) => {
  const [providers, setProviders] = useState<Provider[]>([]);
  const [selectedModel, setSelectedModel] = useState('');

  useEffect(() => {
    fetch('/api/multi-llm/providers')
      .then(res => res.json())
      .then(data => setProviders(data.providers));
  }, []);

  const currentProvider = providers.find(p => p.value === selectedProvider);

  const handleProviderChange = (newProvider: string) => {
    const provider = providers.find(p => p.value === newProvider);
    const defaultModel = provider?.models[0] || '';
    setSelectedModel(defaultModel);
    onProviderChange(newProvider, defaultModel);
  };

  const handleModelChange = (model: string) => {
    setSelectedModel(model);
    onProviderChange(selectedProvider, model);
  };

  return (
    <div className="bg-white p-4 rounded-lg border shadow-sm">
      <h3 className="font-semibold mb-3">AI ëª¨ë¸ ì„ íƒ</h3>
      
      {/* ì œê³µì ì„ íƒ */}
      <div className="grid grid-cols-1 md:grid-cols-3 gap-2 mb-4">
        {providers.map((provider) => (
          <button
            key={provider.value}
            onClick={() => handleProviderChange(provider.value)}
            className={`p-3 border rounded-lg text-left transition-all ${
              selectedProvider === provider.value
                ? 'border-blue-500 bg-blue-50'
                : 'border-gray-200 hover:border-gray-300'
            }`}
          >
            <div className="flex items-center gap-2 mb-1">
              <span className="text-lg">{provider.icon}</span>
              <span className="font-medium text-sm">{provider.label}</span>
            </div>
            <div className="text-xs text-gray-600">
              {provider.strengths.slice(0, 2).join(', ')}
            </div>
          </button>
        ))}
      </div>

      {/* ëª¨ë¸ ì„ íƒ */}
      {currentProvider && currentProvider.models.length > 1 && (
        <div>
          <label className="block text-sm font-medium mb-2">ëª¨ë¸ ì„ íƒ</label>
          <select
            value={selectedModel}
            onChange={(e) => handleModelChange(e.target.value)}
            className="w-full p-2 border rounded text-sm"
          >
            {currentProvider.models.map((model) => (
              <option key={model} value={model}>
                {model}
              </option>
            ))}
          </select>
        </div>
      )}

      {/* ê°•ì  í‘œì‹œ */}
      {currentProvider && (
        <div className="mt-3">
          <div className="text-xs text-gray-600 mb-1">ì´ ëª¨ë¸ì˜ ê°•ì :</div>
          <div className="flex flex-wrap gap-1">
            {currentProvider.strengths.map((strength, idx) => (
              <span
                key={idx}
                className="px-2 py-1 bg-gray-100 text-xs rounded"
              >
                {strength}
              </span>
            ))}
          </div>
        </div>
      )}
    </div>
  );
};

export default LLMProviderSelector;
```

---

## ğŸ“Š **ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ**

### **ëª¨ë‹ˆí„°ë§ ì»´í¬ë„ŒíŠ¸**
```typescript
// src/components/ai/LLMPerformanceDashboard.tsx
import React, { useState, useEffect } from 'react';

interface PerformanceStats {
  provider_stats: Record<string, {
    total_requests: number;
    success_count: number;
    avg_response_time: number;
    last_success: string | null;
  }>;
  recommendation: string;
}

const LLMPerformanceDashboard: React.FC = () => {
  const [stats, setStats] = useState<PerformanceStats | null>(null);

  useEffect(() => {
    const fetchStats = () => {
      fetch('/api/multi-llm/performance')
        .then(res => res.json())
        .then(data => setStats(data));
    };

    fetchStats();
    const interval = setInterval(fetchStats, 30000); // 30ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸
    
    return () => clearInterval(interval);
  }, []);

  if (!stats) return <div>ë¡œë”© ì¤‘...</div>;

  return (
    <div className="bg-white p-6 rounded-lg border">
      <h3 className="text-lg font-semibold mb-4">LLM ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§</h3>
      
      {/* ì¶”ì²œ ì œê³µì */}
      <div className="mb-4 p-3 bg-green-50 rounded-lg">
        <div className="text-sm font-medium text-green-800">
          í˜„ì¬ ìµœê³  ì„±ëŠ¥: {stats.recommendation || 'ì¸¡ì • ì¤‘...'}
        </div>
      </div>

      {/* ì œê³µìë³„ í†µê³„ */}
      <div className="space-y-4">
        {Object.entries(stats.provider_stats).map(([provider, stat]) => {
          const successRate = stat.total_requests > 0 
            ? ((stat.success_count / stat.total_requests) * 100).toFixed(1)
            : '0';

          return (
            <div key={provider} className="border rounded-lg p-4">
              <div className="flex justify-between items-center mb-2">
                <h4 className="font-medium capitalize">{provider}</h4>
                <span className={`px-2 py-1 rounded text-xs ${
                  parseFloat(successRate) > 90 
                    ? 'bg-green-100 text-green-800'
                    : parseFloat(successRate) > 70
                    ? 'bg-yellow-100 text-yellow-800'
                    : 'bg-red-100 text-red-800'
                }`}>
                  ì„±ê³µë¥  {successRate}%
                </span>
              </div>
              
              <div className="grid grid-cols-3 gap-4 text-sm">
                <div>
                  <div className="text-gray-600">ì´ ìš”ì²­</div>
                  <div className="font-medium">{stat.total_requests}</div>
                </div>
                <div>
                  <div className="text-gray-600">í‰ê·  ì‘ë‹µì‹œê°„</div>
                  <div className="font-medium">{stat.avg_response_time.toFixed(2)}ì´ˆ</div>
                </div>
                <div>
                  <div className="text-gray-600">ë§ˆì§€ë§‰ ì„±ê³µ</div>
                  <div className="font-medium">
                    {stat.last_success 
                      ? new Date(stat.last_success).toLocaleTimeString()
                      : 'ì—†ìŒ'
                    }
                  </div>
                </div>
              </div>
            </div>
          );
        })}
      </div>
    </div>
  );
};

export default LLMPerformanceDashboard;
```

---

## ğŸš€ **Quick Setup ê°€ì´ë“œ**

### **1. íŒ¨í‚¤ì§€ ì„¤ì¹˜**
```bash
# ë‹¤ì¤‘ LLM íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install langchain-anthropic langchain-google-genai
pip install anthropic google-generativeai
```

### **2. ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸**
```python
# test_multi_llm.py
import asyncio
from llm.llm_factory import LLMFactory, LLMProvider

async def test_all_providers():
    factory = LLMFactory()
    test_message = "ì•ˆë…•í•˜ì„¸ìš”! ë¦°ì½”ë¦¬ì•„ AI í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤."
    
    for provider in [LLMProvider.OPENAI, LLMProvider.ANTHROPIC, LLMProvider.GOOGLE]:
        try:
            print(f"\nğŸ§ª {provider.value} í…ŒìŠ¤íŠ¸ ì¤‘...")
            llm = factory.create_llm(provider)
            response = await llm.ainvoke(test_message)
            print(f"âœ… {provider.value}: {response.content[:100]}...")
        except Exception as e:
            print(f"âŒ {provider.value}: {e}")

if __name__ == "__main__":
    asyncio.run(test_all_providers())
```

### **3. ì‹¤í–‰ ëª…ë ¹ì–´**
```bash
# ë°±ì—”ë“œ ì‹¤í–‰
cd backend/ai-service
python main.py

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python test_multi_llm.py
```

---

## ğŸ¯ **MIDAS í¬íŠ¸í´ë¦¬ì˜¤ í•µì‹¬ ê°€ì¹˜**

ì´ ë‹¤ì¤‘ LLM ì‹œìŠ¤í…œìœ¼ë¡œ ì¦ëª…í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤:

### **âœ… ìµœì‹  AI ê¸°ìˆ  ë§ˆìŠ¤í„°**
- OpenAI GPT-4, Anthropic Claude, Google Gemini ëª¨ë“  API í™œìš©
- ê° ëª¨ë¸ì˜ ê°•ì ì„ ì´í•´í•˜ê³  ì ì ˆíˆ í™œìš©

### **âœ… ì‹œìŠ¤í…œ ì„¤ê³„ ëŠ¥ë ¥**  
- í™•ì¥ ê°€ëŠ¥í•œ íŒ©í† ë¦¬ íŒ¨í„´ ì ìš©
- ì§€ëŠ¥í˜• í´ë°± ì‹œìŠ¤í…œ êµ¬í˜„
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìµœì í™”

### **âœ… ì‹¤ë¬´ì  ì ‘ê·¼**
- ë¹„ìš© íš¨ìœ¨ì„± ê³ ë ¤ (ì‘ì—…ë³„ ìµœì  ëª¨ë¸ ì„ íƒ)
- ì•ˆì •ì„± í™•ë³´ (ë‹¤ì¤‘ í´ë°± ì§€ì›)
- ì‚¬ìš©ì ê²½í—˜ ìµœì í™”

### **âœ… í™•ì¥ì„± ë° ìœ ì§€ë³´ìˆ˜ì„±**
- ìƒˆë¡œìš´ LLM ì œê³µì ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥
- ëª¨ë“ˆí™”ëœ êµ¬ì¡°ë¡œ ìœ ì§€ë³´ìˆ˜ ìš©ì´
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

**ê²°ê³¼**: **MIDASì—ì„œ ì›í•˜ëŠ” ëª¨ë“  AI/LLM ê¸°ìˆ  ì—­ëŸ‰ì„ ì™„ë²½í•˜ê²Œ ì¦ëª…** ğŸš€

ì´ì œ **3ê°œì˜ ìµœì‹  LLMì„ ëª¨ë‘ í™œìš©í•˜ëŠ” ì‹œìŠ¤í…œ**ìœ¼ë¡œ **ìµœê³  ìˆ˜ì¤€ì˜ í¬íŠ¸í´ë¦¬ì˜¤**ë¥¼ ì™„ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸ’ª 