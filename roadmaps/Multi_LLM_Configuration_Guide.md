# ğŸ¤– ë‹¤ì¤‘ LLM API ì„¤ì • ê°€ì´ë“œ

## ğŸ” **í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**

### **í™˜ê²½ ë³€ìˆ˜ íŒŒì¼ (.env)**
```bash
# backend/ai-service/.env

# === LLM API í‚¤ ì„¤ì • ===
# OpenAI GPT-4/3.5-turbo
OPENAI_API_KEY=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

# Anthropic Claude (Claude-3.5-sonnet, Claude-3-haiku)
ANTHROPIC_API_KEY=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

# Google Gemini Pro 2.5
GOOGLE_API_KEY=AIzaSyBd3vM8XgfrBWZq3J2upUvIwDsIahdf3lE

# === ê¸°ë³¸ ëª¨ë¸ ì„¤ì • ===
DEFAULT_LLM_PROVIDER=openai
DEFAULT_LLM_MODEL=gpt-4
FALLBACK_LLM_PROVIDER=anthropic
FALLBACK_LLM_MODEL=claude-3-5-sonnet-20241022

# === Vector DB ì„¤ì • ===
PINECONE_API_KEY=pcsk_4AQeoW_T5yG4j6JcG2kbvQZjTsrBg5HzyYofmLZUixjrCz3gG4s4sBWVN6TZ819BzNzCY8

# === Database ì„¤ì • ===
SUPABASE_URL=https://fpyqjvnpduwifxgnbrck.supabase.co
SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZweXFqdm5wZHV3aWZ4Z25icmNrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDk1NDA0MDQsImV4cCI6MjA2NTExNjQwNH0.Uqn7ajeRHEqMl7gbPtOYHKd1ZhUb6xiMTZsK4QMxtfU

# === í”„ë¡ íŠ¸ì—”ë“œìš© (Next.js) ===
NEXT_PUBLIC_SUPABASE_URL=https://fpyqjvnpduwifxgnbrck.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZweXFqdm5wZHV3aWZ4Z25icmNrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDk1NDA0MDQsImV4cCI6MjA2NTExNjQwNH0.Uqn7ajeRHEqMl7gbPtOYHKd1ZhUb6xiMTZsK4QMxtfU
ENVIRONMENT=development
LOG_LEVEL=INFO
```

---

## ğŸ”§ **ë‹¤ì¤‘ LLM ê´€ë¦¬ì êµ¬í˜„**

### **LLM Factory í´ë˜ìŠ¤**

```python
# backend/ai-service/llm/llm_factory.py
from typing import Optional
from enum import Enum
import os
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

class LLMProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"

class LLMFactory:
    def __init__(self):
        self.api_keys = {
            LLMProvider.OPENAI: os.getenv("OPENAI_API_KEY"),
            LLMProvider.ANTHROPIC: os.getenv("ANTHROPIC_API_KEY"),
            LLMProvider.GOOGLE: os.getenv("GOOGLE_API_KEY"),
        }
    
    def create_llm(self, provider: LLMProvider, model: str = None, **kwargs):
        """LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        if provider == LLMProvider.OPENAI:
            return ChatOpenAI(
                api_key=self.api_keys[provider],
                model=model or "gpt-4",
                **kwargs
            )
        elif provider == LLMProvider.ANTHROPIC:
            return ChatAnthropic(
                api_key=self.api_keys[provider],
                model=model or "claude-3-5-sonnet-20241022",
                **kwargs
            )
        elif provider == LLMProvider.GOOGLE:
            return ChatGoogleGenerativeAI(
                google_api_key=self.api_keys[provider],
                model=model or "gemini-pro",
                **kwargs
            )
```

### **ë‹¤ì¤‘ LLM ì±—ë´‡**

```python
# backend/ai-service/ai/multi_llm_chatbot.py
from llm.llm_factory import LLMFactory, LLMProvider

class MultiLLMChatBot:
    def __init__(self):
        self.factory = LLMFactory()
        self.default_provider = LLMProvider.OPENAI
    
    async def get_response(self, message: str, provider: str = None):
        """ì„ íƒëœ LLMìœ¼ë¡œ ì‘ë‹µ ìƒì„±"""
        try:
            # ì œê³µì ì„ íƒ
            selected_provider = LLMProvider(provider) if provider else self.default_provider
            
            # LLM ìƒì„±
            llm = self.factory.create_llm(selected_provider)
            
            # ì‘ë‹µ ìƒì„±
            response = await llm.ainvoke(message)
            
            return {
                "response": response.content,
                "provider_used": selected_provider.value,
                "success": True
            }
        except Exception as e:
            # í´ë°± ì²˜ë¦¬
            return await self._fallback_response(message, str(e))
    
    async def _fallback_response(self, message: str, error: str):
        """í´ë°± LLMìœ¼ë¡œ ì‘ë‹µ"""
        for provider in [LLMProvider.ANTHROPIC, LLMProvider.GOOGLE, LLMProvider.OPENAI]:
            try:
                llm = self.factory.create_llm(provider)
                response = await llm.ainvoke(message)
                return {
                    "response": response.content,
                    "provider_used": provider.value,
                    "fallback": True,
                    "original_error": error
                }
            except:
                continue
        
        return {
            "response": "ì£„ì†¡í•©ë‹ˆë‹¤. ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "success": False,
            "error": error
        }
```

---

## ğŸ¨ **í”„ë¡ íŠ¸ì—”ë“œ LLM ì„ íƒ UI**

```typescript
// src/components/ai/LLMSelector.tsx
import React, { useState } from 'react';

interface LLMSelectorProps {
  onProviderChange: (provider: string) => void;
}

const LLMSelector: React.FC<LLMSelectorProps> = ({ onProviderChange }) => {
  const [selectedProvider, setSelectedProvider] = useState('openai');

  const providers = [
    { value: 'openai', label: 'OpenAI GPT-4', icon: 'ğŸ¤–' },
    { value: 'anthropic', label: 'Claude 3.5 Sonnet', icon: 'ğŸ§ ' },
    { value: 'google', label: 'Gemini Pro', icon: 'âœ¨' }
  ];

  const handleChange = (provider: string) => {
    setSelectedProvider(provider);
    onProviderChange(provider);
  };

  return (
    <div className="flex gap-2 p-2 bg-gray-100 rounded-lg">
      {providers.map((provider) => (
        <button
          key={provider.value}
          onClick={() => handleChange(provider.value)}
          className={`flex items-center gap-2 px-3 py-2 rounded transition-colors ${
            selectedProvider === provider.value
              ? 'bg-blue-500 text-white'
              : 'bg-white text-gray-700 hover:bg-gray-50'
          }`}
        >
          <span>{provider.icon}</span>
          <span className="text-sm font-medium">{provider.label}</span>
        </button>
      ))}
    </div>
  );
};

export default LLMSelector;
```

---

## ğŸ“Š **API ì—”ë“œí¬ì¸íŠ¸**

```python
# backend/ai-service/api/llm_endpoints.py
from fastapi import APIRouter, HTTPException
from ai.multi_llm_chatbot import MultiLLMChatBot

router = APIRouter(prefix="/api/llm", tags=["Multi-LLM"])
chatbot = MultiLLMChatBot()

@router.get("/providers")
async def get_available_providers():
    """ì‚¬ìš© ê°€ëŠ¥í•œ LLM ì œê³µì ëª©ë¡"""
    return {
        "providers": [
            {"value": "openai", "label": "OpenAI GPT-4"},
            {"value": "anthropic", "label": "Claude 3.5 Sonnet"},
            {"value": "google", "label": "Google Gemini Pro"}
        ]
    }

@router.post("/chat")
async def chat_with_llm(message: str, provider: str = "openai"):
    """ì„ íƒëœ LLMê³¼ ì±„íŒ…"""
    try:
        result = await chatbot.get_response(message, provider)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

---

## ğŸ”„ **ì—…ë°ì´íŠ¸ëœ main.py**

```python
# backend/ai-service/main.py ì—…ë°ì´íŠ¸
from api.llm_endpoints import router as llm_router

# ë¼ìš°í„° ì¶”ê°€
app.include_router(llm_router)

@app.websocket("/ws/chat/{provider}")
async def websocket_chat(websocket: WebSocket, provider: str = "openai"):
    await websocket.accept()
    chatbot = MultiLLMChatBot()
    
    try:
        while True:
            message = await websocket.receive_text()
            result = await chatbot.get_response(message, provider)
            await websocket.send_json(result)
    except WebSocketDisconnect:
        pass
```

---

## ğŸ›¡ï¸ **ë³´ì•ˆ ë° ì‚¬ìš©ëŸ‰ ê´€ë¦¬**

### **API í‚¤ ë³´ì•ˆ**
```bash
# .env íŒŒì¼ì„ .gitignoreì— ì¶”ê°€
echo ".env" >> .gitignore
echo "*.key" >> .gitignore
```

### **ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§**
```python
# backend/ai-service/monitoring/usage_tracker.py
class LLMUsageTracker:
    def __init__(self):
        self.usage_stats = {}
    
    def track_request(self, provider: str, tokens: int = 0):
        """ìš”ì²­ ì¶”ì """
        if provider not in self.usage_stats:
            self.usage_stats[provider] = {"requests": 0, "tokens": 0}
        
        self.usage_stats[provider]["requests"] += 1
        self.usage_stats[provider]["tokens"] += tokens
    
    def get_stats(self):
        """ì‚¬ìš© í†µê³„ ë°˜í™˜"""
        return self.usage_stats
```

---

## ğŸš€ **Quick Start ì—…ë°ì´íŠ¸**

### **íŒ¨í‚¤ì§€ ì„¤ì¹˜**
```bash
# ì¶”ê°€ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install langchain-anthropic langchain-google-genai
```

### **ì¦‰ì‹œ í…ŒìŠ¤íŠ¸**
```python
# í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
from llm.llm_factory import LLMFactory, LLMProvider

async def test_all_llms():
    factory = LLMFactory()
    
    for provider in [LLMProvider.OPENAI, LLMProvider.ANTHROPIC, LLMProvider.GOOGLE]:
        try:
            llm = factory.create_llm(provider)
            response = await llm.ainvoke("ì•ˆë…•í•˜ì„¸ìš”!")
            print(f"âœ… {provider.value}: {response.content}")
        except Exception as e:
            print(f"âŒ {provider.value}: {e}")

# ì‹¤í–‰
import asyncio
asyncio.run(test_all_llms())
```

---

## ğŸ¯ **MiDS í¬íŠ¸í´ë¦¬ì˜¤ ê°•ì **

ì´ ë‹¤ì¤‘ LLM ì‹œìŠ¤í…œìœ¼ë¡œ ë³´ì—¬ì¤„ ìˆ˜ ìˆëŠ” ê²ƒë“¤:

âœ… **ìµœì‹  AI ê¸°ìˆ  ìŠ¤íƒ í™œìš©**
- OpenAI GPT-4, Anthropic Claude, Google Gemini

âœ… **ì•ˆì •ì„±ê³¼ í™•ì¥ì„±**
- ìë™ í´ë°± ì‹œìŠ¤í…œ
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

âœ… **ì‹¤ë¬´ì  ì ‘ê·¼**
- ë¹„ìš© ìµœì í™”
- ì‚¬ìš©ëŸ‰ ì¶”ì 

âœ… **ìœ ì—°í•œ ì•„í‚¤í…ì²˜**
- ìƒˆë¡œìš´ LLM ì‰½ê²Œ ì¶”ê°€ ê°€ëŠ¥
- ì œê³µìë³„ íŠ¹ì„± í™œìš©

ì´ì œ **3ê°œì˜ ì£¼ìš” LLMì„ ëª¨ë‘ í™œìš©í•˜ëŠ” ì‹œìŠ¤í…œ**ìœ¼ë¡œ MiDSì—ì„œ ìµœê³  ìˆ˜ì¤€ì˜ AI/LLM ê°œë°œ ì—­ëŸ‰ì„ ì¦ëª…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€ 