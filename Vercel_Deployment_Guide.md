# ğŸš€ Vercel ë°°í¬ ìµœì í™” ê°€ì´ë“œ

## ğŸ¯ **Vercel ì œì•½ì‚¬í•­ ë¶„ì„ ë° í•´ê²°ì±…**

### **âš ï¸ Vercel ì œì•½ì‚¬í•­**
- **Serverless Function ì‹¤í–‰ ì‹œê°„**: ë¬´ë£Œ 10ì´ˆ, Pro 60ì´ˆ
- **ë©”ëª¨ë¦¬ ì œí•œ**: ìµœëŒ€ 1GB (Pro í”Œëœ)
- **WebSocket ì œí•œ**: ì§€ì†ì  ì—°ê²° ì–´ë ¤ì›€
- **Python ì§€ì›**: ì œí•œì  (Node.js/Edge Runtime ê¶Œì¥)
- **íŒŒì¼ ì‹œìŠ¤í…œ**: Read-only (ì„ì‹œ íŒŒì¼ë§Œ ê°€ëŠ¥)

### **âœ… ìµœì í™”ëœ í•´ê²°ì±…**

---

## ğŸ”„ **ìˆ˜ì •ëœ ì•„í‚¤í…ì²˜**

### **1. ë°±ì—”ë“œ â†’ Next.js API Routes ì „í™˜**
```typescript
// pages/api/chat/[provider].ts
import { NextApiRequest, NextApiResponse } from 'next';
import OpenAI from 'openai';
import Anthropic from '@anthropic-ai/sdk';
import { GoogleGenerativeAI } from '@google/generative-ai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

const anthropic = new Anthropic({
  apiKey: process.env.ANTHROPIC_API_KEY,
});

const googleAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY!);

export default async function handler(
  req: NextApiRequest,
  res: NextApiResponse
) {
  const { provider } = req.query;
  const { message } = req.body;

  try {
    let response;
    
    switch (provider) {
      case 'openai':
        const completion = await openai.chat.completions.create({
          model: "gpt-4",
          messages: [{ role: "user", content: message }],
          max_tokens: 1000,
        });
        response = completion.choices[0].message.content;
        break;
        
      case 'anthropic':
        const claudeResponse = await anthropic.messages.create({
          model: "claude-3-5-sonnet-20241022",
          max_tokens: 1000,
          messages: [{ role: "user", content: message }],
        });
        response = claudeResponse.content[0].text;
        break;
        
      case 'google':
        const model = googleAI.getGenerativeModel({ model: "gemini-pro" });
        const result = await model.generateContent(message);
        response = result.response.text();
        break;
        
      default:
        throw new Error('Unsupported provider');
    }

    res.status(200).json({ success: true, response, provider });
  } catch (error) {
    res.status(500).json({ success: false, error: error.message });
  }
}
```

### **2. LangChain.js ê¸°ë°˜ RAG ì‹œìŠ¤í…œ**
```typescript
// lib/rag-system.ts
import { OpenAIEmbeddings } from '@langchain/openai';
import { MemoryVectorStore } from 'langchain/vectorstores/memory';
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';
import { Document } from 'langchain/document';

export class VercelRAGSystem {
  private vectorStore: MemoryVectorStore;
  private embeddings: OpenAIEmbeddings;

  constructor() {
    this.embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY,
    });
  }

  async initializeVectorStore() {
    // Supabaseì—ì„œ ì œí’ˆ ë°ì´í„° ë¡œë“œ
    const response = await fetch(`${process.env.SUPABASE_URL}/rest/v1/product_introductions`, {
      headers: {
        'apikey': process.env.SUPABASE_ANON_KEY!,
        'Authorization': `Bearer ${process.env.SUPABASE_ANON_KEY}`,
      },
    });
    
    const products = await response.json();
    
    // ë¬¸ì„œë¡œ ë³€í™˜
    const documents = products.map((product: any) => new Document({
      pageContent: `ì œí’ˆëª…: ${product.name}\nì„¤ëª…: ${product.description}\níŠ¹ì§•: ${product.features?.join(', ')}`,
      metadata: { source: 'products', id: product.id },
    }));

    // í…ìŠ¤íŠ¸ ë¶„í• 
    const textSplitter = new RecursiveCharacterTextSplitter({
      chunkSize: 1000,
      chunkOverlap: 200,
    });
    
    const splits = await textSplitter.splitDocuments(documents);
    
    // ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    this.vectorStore = await MemoryVectorStore.fromDocuments(splits, this.embeddings);
  }

  async similaritySearch(query: string, k: number = 4) {
    if (!this.vectorStore) {
      await this.initializeVectorStore();
    }
    
    return await this.vectorStore.similaritySearch(query, k);
  }
}
```

### **3. Server-Sent Events (SSE) ìŠ¤íŠ¸ë¦¬ë°**
```typescript
// pages/api/chat/stream.ts
import { NextApiRequest, NextApiResponse } from 'next';
import OpenAI from 'openai';

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  const { message, provider = 'openai' } = req.body;

  // SSE í—¤ë” ì„¤ì •
  res.writeHead(200, {
    'Content-Type': 'text/event-stream',
    'Cache-Control': 'no-cache',
    'Connection': 'keep-alive',
    'Access-Control-Allow-Origin': '*',
  });

  try {
    const openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY,
    });

    const stream = await openai.chat.completions.create({
      model: "gpt-4",
      messages: [{ role: "user", content: message }],
      stream: true,
      max_tokens: 1000,
    });

    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content || '';
      if (content) {
        res.write(`data: ${JSON.stringify({ content, done: false })}\n\n`);
      }
    }

    res.write(`data: ${JSON.stringify({ content: '', done: true })}\n\n`);
    res.end();
  } catch (error) {
    res.write(`data: ${JSON.stringify({ error: error.message, done: true })}\n\n`);
    res.end();
  }
}
```

### **4. Edge Functions í™œìš©**
```typescript
// pages/api/edge/llm-router.ts
import { NextRequest } from 'next/server';

export const config = {
  runtime: 'edge',
};

export default async function handler(req: NextRequest) {
  const { message, taskType } = await req.json();
  
  // ì‘ì—… ìœ í˜•ë³„ ìµœì  LLM ì„ íƒ
  const providerMap = {
    'coding': 'openai',
    'reasoning': 'anthropic', 
    'multilingual': 'google',
    'general': 'openai'
  };
  
  const selectedProvider = providerMap[taskType] || 'openai';
  
  // ì„ íƒëœ LLMìœ¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
  const response = await fetch(`${req.nextUrl.origin}/api/chat/${selectedProvider}`, {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ message }),
  });
  
  return response;
}
```

---

## ğŸ¨ **í”„ë¡ íŠ¸ì—”ë“œ ìŠ¤íŠ¸ë¦¬ë° ì»´í¬ë„ŒíŠ¸**

### **SSE ê¸°ë°˜ ì±„íŒ… ì»´í¬ë„ŒíŠ¸**
```typescript
// components/StreamingChat.tsx
import { useState, useEffect } from 'react';

export default function StreamingChat() {
  const [messages, setMessages] = useState<Array<{role: string, content: string}>>([]);
  const [input, setInput] = useState('');
  const [isStreaming, setIsStreaming] = useState(false);

  const sendMessage = async () => {
    if (!input.trim()) return;
    
    const userMessage = { role: 'user', content: input };
    setMessages(prev => [...prev, userMessage]);
    setInput('');
    setIsStreaming(true);

    try {
      const response = await fetch('/api/chat/stream', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: input }),
      });

      const reader = response.body?.getReader();
      let aiMessage = { role: 'ai', content: '' };
      setMessages(prev => [...prev, aiMessage]);

      if (reader) {
        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          const chunk = new TextDecoder().decode(value);
          const lines = chunk.split('\n');
          
          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = JSON.parse(line.slice(6));
              if (data.content) {
                setMessages(prev => {
                  const newMessages = [...prev];
                  newMessages[newMessages.length - 1].content += data.content;
                  return newMessages;
                });
              }
              if (data.done) break;
            }
          }
        }
      }
    } catch (error) {
      console.error('Streaming error:', error);
    } finally {
      setIsStreaming(false);
    }
  };

  return (
    <div className="max-w-2xl mx-auto p-4">
      <div className="space-y-4 mb-4 h-96 overflow-y-auto">
        {messages.map((message, index) => (
          <div key={index} className={`flex ${message.role === 'user' ? 'justify-end' : 'justify-start'}`}>
            <div className={`max-w-xs lg:max-w-md px-4 py-2 rounded-lg ${
              message.role === 'user' 
                ? 'bg-blue-500 text-white' 
                : 'bg-gray-200 text-gray-800'
            }`}>
              {message.content}
              {message.role === 'ai' && isStreaming && index === messages.length - 1 && (
                <span className="animate-pulse">â–‹</span>
              )}
            </div>
          </div>
        ))}
      </div>
      
      <div className="flex gap-2">
        <input
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyPress={(e) => e.key === 'Enter' && sendMessage()}
          className="flex-1 border rounded px-3 py-2"
          placeholder="ë©”ì‹œì§€ ì…ë ¥..."
          disabled={isStreaming}
        />
        <button 
          onClick={sendMessage}
          disabled={isStreaming}
          className="bg-blue-500 text-white px-4 py-2 rounded hover:bg-blue-600 disabled:opacity-50"
        >
          ì „ì†¡
        </button>
      </div>
    </div>
  );
}
```

---

## ğŸ“Š **ì„±ëŠ¥ ìµœì í™” ì „ëµ**

### **1. ìºì‹± ì‹œìŠ¤í…œ**
```typescript
// lib/cache.ts
const responseCache = new Map<string, {response: string, timestamp: number}>();
const CACHE_DURATION = 5 * 60 * 1000; // 5ë¶„

export function getCachedResponse(key: string): string | null {
  const cached = responseCache.get(key);
  if (cached && Date.now() - cached.timestamp < CACHE_DURATION) {
    return cached.response;
  }
  return null;
}

export function setCachedResponse(key: string, response: string) {
  responseCache.set(key, { response, timestamp: Date.now() });
}
```

### **2. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”**
```typescript
// pages/api/batch/inquiries.ts
export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  const { inquiries } = req.body;
  
  // ë™ì‹œ ì²˜ë¦¬ ì œí•œ (Vercel í•œê³„ ê³ ë ¤)
  const BATCH_SIZE = 5;
  const results = [];
  
  for (let i = 0; i < inquiries.length; i += BATCH_SIZE) {
    const batch = inquiries.slice(i, i + BATCH_SIZE);
    const batchPromises = batch.map(processInquiry);
    const batchResults = await Promise.allSettled(batchPromises);
    results.push(...batchResults);
  }
  
  res.json({ results });
}
```

---

## ğŸš€ **ë°°í¬ ìµœì í™”**

### **1. vercel.json ì„¤ì •**
```json
{
  "functions": {
    "pages/api/**/*.ts": {
      "maxDuration": 60
    }
  },
  "env": {
    "OPENAI_API_KEY": "@openai_api_key",
    "ANTHROPIC_API_KEY": "@anthropic_api_key",
    "GOOGLE_API_KEY": "@google_api_key",
    "SUPABASE_URL": "@supabase_url",
    "SUPABASE_ANON_KEY": "@supabase_anon_key"
  },
  "rewrites": [
    {
      "source": "/api/chat/:provider",
      "destination": "/api/chat/[provider]"
    }
  ]
}
```

### **2. í™˜ê²½ë³€ìˆ˜ ì„¤ì •**
```bash
# Vercel í™˜ê²½ë³€ìˆ˜ ì„¤ì •
vercel env add OPENAI_API_KEY
vercel env add ANTHROPIC_API_KEY  
vercel env add GOOGLE_API_KEY
vercel env add SUPABASE_URL
vercel env add SUPABASE_ANON_KEY
```

---

## ğŸ“‹ **ìˆ˜ì •ëœ 24ì¼ ë¡œë“œë§µ**

### **Week 1 (6/20-6/26): Next.js AI ì±—ë´‡**
- **Day 1-2**: Next.js API Routes + ë‹¤ì¤‘ LLM ì„¤ì •
- **Day 3-4**: LangChain.js RAG ì‹œìŠ¤í…œ 
- **Day 5-7**: SSE ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ… + ìµœì í™”

### **Week 2 (6/27-7/3): Serverless ì›Œí¬í”Œë¡œìš°**
- **Day 8-9**: Edge Functions ê¸°ë°˜ LLM ë¼ìš°í„°
- **Day 10-11**: Q&A ìë™í™” (ë°°ì¹˜ ì²˜ë¦¬)
- **Day 12-14**: Supabase ì—°ë™ + ì„±ëŠ¥ ìµœì í™”

### **Week 3 (7/4-7/10): ê²¬ì  ì‹œìŠ¤í…œ**
- **Day 15-16**: AI ê²¬ì  ìƒì„± API
- **Day 17-18**: PDF ìƒì„± (í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ)
- **Day 19-21**: ì „ì²´ ì‹œìŠ¤í…œ í†µí•©

### **Week 4 (7/11-7/13): ì™„ì„± & ë°°í¬**
- **Day 22**: ë¬¸ì„œ ì²˜ë¦¬ ìµœì í™”
- **Day 23**: Vercel ë°°í¬ + ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- **Day 24**: í¬íŠ¸í´ë¦¬ì˜¤ ì™„ì„±

---

## âœ… **Vercelë¡œ ê°€ëŠ¥í•œ ê²ƒë“¤**

1. **âœ… ë‹¤ì¤‘ LLM ì‹œìŠ¤í…œ** - JavaScript SDK ì‚¬ìš©
2. **âœ… ì‹¤ì‹œê°„ ì±„íŒ…** - SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ëŒ€ì²´
3. **âœ… RAG ê²€ìƒ‰** - LangChain.js + MemoryVectorStore
4. **âœ… ìë™í™” ì›Œí¬í”Œë¡œìš°** - Edge Functions í™œìš©
5. **âœ… ê²¬ì  ì‹œìŠ¤í…œ** - í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ PDF ìƒì„±
6. **âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§** - Vercel Analytics í†µí•©

---

## ğŸ¯ **ìµœì¢… ê¶Œì¥ì‚¬í•­**

### **ğŸ‘ Vercel ì „ìš© ê°œë°œ (ì¶”ì²œ)**
- **ì¥ì **: ë°°í¬ ê°„ë‹¨, í™•ì¥ì„± ì¢‹ìŒ, ë¬´ë£Œ ì‹œì‘ ê°€ëŠ¥
- **ë‹¨ì **: ì¼ë¶€ ê¸°ëŠ¥ ì œì•½, í•™ìŠµ ê³¡ì„ 

### **ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼**
- **í”„ë¡ íŠ¸ì—”ë“œ**: Vercel (Next.js)
- **AI ë°±ì—”ë“œ**: Railway/Render (FastAPI Python)
- **ì¥ì **: ìµœê³  ì„±ëŠ¥, ëª¨ë“  ê¸°ëŠ¥ êµ¬í˜„ ê°€ëŠ¥

**ê²°ë¡ **: **Vercel ì „ìš©ìœ¼ë¡œë„ ì¶©ë¶„íˆ ì¸ìƒì ì¸ í¬íŠ¸í´ë¦¬ì˜¤ êµ¬í˜„ ê°€ëŠ¥**í•˜ì§€ë§Œ, ì¼ë¶€ ì•„í‚¤í…ì²˜ ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤!

# Vercel ë°°í¬ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

## ğŸš¨ í•˜ì–€ í™”ë©´ ë¬¸ì œ í•´ê²° ë°©ë²•

### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ê°€ì¥ ì¤‘ìš”!)

Vercel ëŒ€ì‹œë³´ë“œì—ì„œ ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ë¥¼ **ë°˜ë“œì‹œ** ì„¤ì •í•˜ì„¸ìš”:

```
VITE_SUPABASE_URL=https://fpyqjvnpduwifxgnbrck.supabase.co
VITE_SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImZweXFqdm5wZHV3aWZ4Z25icmNrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDk1NDA0MDQsImV4cCI6MjA2NTExNjQwNH0.Uqn7ajeRHEqMl7gbPtOYHKd1ZhUb6xiMTZsK4QMxtfU
NODE_ENV=production
```

### 2. createPortal ì˜¤ë¥˜ ìˆ˜ì •

ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ëª¨ë“  createPortal ì˜¤ë¥˜ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”:

```bash
# Windows (PowerShell)
Get-ChildItem -Path "src" -Include "*.tsx" -Recurse | ForEach-Object {
    $content = Get-Content $_.FullName -Raw
    if ($content -match "createPortal\(" -and $content -notmatch "document\.body") {
        $content = $content -replace "(\s+)\);?\s*$", "`$1, document.body);"
        Set-Content -Path $_.FullName -Value $content
        Write-Host "Fixed: $($_.FullName)"
    }
}
```

### 3. ë¹Œë“œ ë° ë°°í¬ í™•ì¸

```bash
# ë¡œì»¬ì—ì„œ í”„ë¡œë•ì…˜ ë¹Œë“œ í…ŒìŠ¤íŠ¸
npm run build
npm run preview

# ë¹Œë“œ ì„±ê³µ í›„ Vercelì— ë°°í¬
vercel --prod
```

### 4. ë°°í¬ í›„ í™•ì¸ì‚¬í•­

1. **ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬ ì½˜ì†” í™•ì¸**
   - JavaScript ì˜¤ë¥˜ê°€ ì—†ëŠ”ì§€ í™•ì¸
   - 404 ì˜¤ë¥˜ê°€ ì—†ëŠ”ì§€ í™•ì¸

2. **ë„¤íŠ¸ì›Œí¬ íƒ­ í™•ì¸**
   - ëª¨ë“  ë¦¬ì†ŒìŠ¤ê°€ ë¡œë“œë˜ëŠ”ì§€ í™•ì¸
   - API ìš”ì²­ì´ ì„±ê³µí•˜ëŠ”ì§€ í™•ì¸

3. **Vercel í•¨ìˆ˜ ë¡œê·¸ í™•ì¸**
   - Vercel ëŒ€ì‹œë³´ë“œì—ì„œ í•¨ìˆ˜ ë¡œê·¸ í™•ì¸

### 5. ì¼ë°˜ì ì¸ ë¬¸ì œ í•´ê²°

#### ë¬¸ì œ: í•˜ì–€ í™”ë©´ë§Œ ë³´ì„
**í•´ê²°ì±…:**
1. Vercel í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í™•ì¸
2. ë¸Œë¼ìš°ì € ìºì‹œ ì‚­ì œ í›„ ì¬ì‹œë„
3. ì‹œí¬ë¦¿ ëª¨ë“œì—ì„œ ì ‘ì† í™•ì¸

#### ë¬¸ì œ: "Target container is not a DOM element"
**í•´ê²°ì±…:**
1. ëª¨ë“  createPortal í˜¸ì¶œì— `document.body` ì¶”ê°€
2. SSR ì•ˆì „ì„± í™•ì¸ (`typeof window !== 'undefined'` ì²´í¬)

#### ë¬¸ì œ: í™˜ê²½ ë³€ìˆ˜ ì˜¤ë¥˜
**í•´ê²°ì±…:**
1. Vercel ëŒ€ì‹œë³´ë“œì—ì„œ í™˜ê²½ ë³€ìˆ˜ ì¬ì„¤ì •
2. ì¬ë°°í¬ ì‹¤í–‰

### 6. ê¸´ê¸‰ ë³µêµ¬ ë°©ë²•

ë§Œì•½ ë°°í¬ê°€ ê³„ì† ì‹¤íŒ¨í•œë‹¤ë©´:

1. **ë¡¤ë°±**: ì´ì „ ì •ìƒ ë°°í¬ë¡œ ë¡¤ë°±
2. **ë¡œì»¬ ë¹Œë“œ í…ŒìŠ¤íŠ¸**: `npm run build && npm run preview`
3. **ë‹¨ê³„ë³„ ë°°í¬**: ì‘ì€ ë³€ê²½ì‚¬í•­ë¶€í„° ì ì§„ì ìœ¼ë¡œ ë°°í¬
4. **í™˜ê²½ ë¶„ë¦¬**: development ë¸Œëœì¹˜ì—ì„œ ë¨¼ì € í…ŒìŠ¤íŠ¸

### 7. ëª¨ë‹ˆí„°ë§ ì„¤ì •

```javascript
// src/lib/error-monitoring.ts
export const initErrorMonitoring = () => {
  if (typeof window !== 'undefined') {
    window.addEventListener('error', (event) => {
      console.error('Global error:', event.error);
      // ì—ëŸ¬ ë¦¬í¬íŒ… ì„œë¹„ìŠ¤ì— ì „ì†¡
    });
    
    window.addEventListener('unhandledrejection', (event) => {
      console.error('Unhandled promise rejection:', event.reason);
      // ì—ëŸ¬ ë¦¬í¬íŒ… ì„œë¹„ìŠ¤ì— ì „ì†¡
    });
  }
};
```

---

## ğŸ¯ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] Vercel í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ì™„ë£Œ
- [ ] ë¡œì»¬ í”„ë¡œë•ì…˜ ë¹Œë“œ ì„±ê³µ
- [ ] createPortal ì˜¤ë¥˜ ìˆ˜ì • ì™„ë£Œ
- [ ] ë°°í¬ í›„ ì •ìƒ ì‘ë™ í™•ì¸
- [ ] ë¸Œë¼ìš°ì € ì½˜ì†” ì˜¤ë¥˜ ì—†ìŒ
- [ ] ëª¨ë“  í˜ì´ì§€ ì •ìƒ ë¡œë“œ í™•ì¸

ì´ ê°€ì´ë“œë¥¼ ë”°ë¥´ë©´ Vercel ë°°í¬ ì‹œ í•˜ì–€ í™”ë©´ ë¬¸ì œë¥¼ ì™„ì „íˆ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 